%!TEX root = ../dissertation.tex

\chapter{Embedded reconstruction of local primitives}
\label{chp:retina}
%\epigraph{We here propose to do just what Copernicus did in attempting to explain the celestial movements. When he found that he could make no further progress by assuming that all the heavenly bodies revolved round the spectator, he reversed the process and tried the experiment of assuming the spectator revolved, while the stars remained at rest.}{Immanuel Kant, \textit{Critique of Pure Reason}}
%\textit{Iniziare la ricostruzione a tempo 0 in maniera embedded durante il readout}

The LHCb collaboration has made of ``real-time analysis" one of his trademark, being the first HEP experiment to implement a fully online software-based trigger. However, even if the RTA project showed outstanding results, this idea relies mostly on leveraging the power of parallel computing such as the one of GPUs. 
At an operational level, the real-time reconstruction described in Section \ref{sec:rta} run the same algorithms that are usually run offline for the reconstruction but in a much faster way. This processing relies heavily on centralized servers, i.e. farms of GPUs and CPUs. This approach, however, at the core-level is resource-intensive and time-consuming, leading to potential bottlenecks in data processing. With Moore's Law showing signs of saturation, traditional centralized-based processing is becoming less viable for tasks that demand rapid and efficient computation.

An alternative to this centralized model is ``edge computing". This philosophy shifts computational tasks from centralized servers to locations closer to the data source, such as directly within or near the detector. The key principle is to start the reconstruction in an embedded way directly in the readout of the detector. Currently, raw data (like a sequence of hits on the VELO) is read and then reassembled to extract meaningful information\cite{https://doi.org/10.5281/zenodo.8119731}. If smarter algorithms could be developed to exploit only the relevant data—such as luminosity, detector position, or beamline position—it would make the process more efficient by reducing unnecessary computation. While this approach may sacrifice some flexibility, it could dramatically speed up processing and focus resources on the data that truly matters.

This chapter explores the concept of ``embedded local primitives," which are raw-like data points that align closely with the specific quantities that need to be measured. The central module for enabling this approach is the Field Programmable Gate Array (FPGA), which allows for rapid computation of these primitives. A prime example of how this technique is already aiding in track reconstruction is the RETINA project. The first phase of the RETINA algorithm involves a $2$D-clustering phase entirely developed in firmware, which is central to this chapter, as it is a crucial step in defining the primitive at the heart of this study.

Finally, I will introduce the specific mother of all the primitives used in this thesis: the counters developed on the VELO detector. These counters enable real-time measurements of luminosity, beamline position, and VELO detector position. By embedding the computation directly within the detector hardware, these counters offer a swift and efficient means of processing data on the spot, minimizing latency and improving the speed and accuracy of crucial measurements. 


\section[The Field Programmable Gate Array]{The Field Programmable Gate Array $\bigl($FPGA$\bigr)$}

In modern digital systems, traditional interconnected integrated circuits are becoming impractical due to their high cost, low efficiency, and reliability issues. Instead, many complex digital systems are implemented on custom integrated circuits designed for specific applications, known as Application Specific Integrated Circuits (ASICs). While ASICs are efficient for mass production, they lack flexibility, especially in experimental or research settings where design changes and rapid iterations are frequent. This is where Field-Programmable Gate Arrays (FPGAs) become invaluable.

FPGAs are integrated circuits that allow users to configure their internal connections, creating custom logic designs. Unlike CPUs and GPUs, which have fixed architectures and execute sequential instructions from memory, FPGAs consist of a matrix of programmable logic blocks that can be interconnected according to a user-defined design. This unique characteristic makes FPGAs highly adaptable, enabling rapid prototyping and experimentation without the need for expensive custom ASIC fabrication.

FPGAs offer several advantages in such environments, where parallel processing and low-latency data handling are critical. Their flexibility allows for embedded reconstruction of local primitives during the readout phase, enabling real-time data processing.

Intel (and previously Altera) is one of the leading FPGA manufacturers. Their Field Programmable Gate Arrays are used extensively in LHCb. The programmable logic blocks within Intel's FPGAs are called Adaptive Logic Modules (ALMs), comprising Lookup Tables (LUTs), full adders, D-type flip-flops, and multiplexers. These components can be configured to perform various digital logic functions, arithmetic operations, and data storage. The number of ALMs in an FPGA determines its processing capacity, with the smallest devices used for the purposes of this thesis containing approximately 170,000 ALMs and the largest about 930,000.

FPGAs offer unique benefits through their pipeline architecture, where multiple tasks can be executed in parallel across different logic blocks. This parallelism significantly increases throughput, allowing for high-speed data processing. Additionally, FPGAs can include Hard Intellectual Property blocks, which are optimized for specific functions like memory or high-speed communication.

The flexibility of FPGAs is further enhanced by hardware description languages (HDLs) like VHDL and Verilog, which allow users to design complex logic structures. The compilation process involves synthesis, place-and-route, and timing analysis to ensure the design runs at the required frequency.

\section{The RETINA Project}
%In the context of advancing technologies for real-time track reconstruction, the INFN-RETINA project has emerged with a focus on implementing an FPGA-based computing architecture inspired by mammalian brain early image reconstruction processes. Progressing beyond its initial research phase, for Run3 operations the project has now developed a demonstrator on real hardware for reconstructing data from the VELO. 
%The initial phase involving the reconstruction of particle hits out of the raw pixel data, is performed by means of a 2D cluster-finding algorithm implemented in the 52 readout FPGAs of the VELO, for a total of 104 parallel channels. Each channel further subdivides clusters among multiple parallel sub-channels. Utilizing a pipelined internal architecture, certain operations are executed swiftly "on the fly" for all hits detected in each event, without causing delays in data acquisition or requiring extensive computational resources, provided the firmware is appropriately designed. Notably, this efficiency is maintained despite an average hit rate of approximately $4 \cdot 10^{10}$/s, which may appear prohibitive for exhaustive processing.\\

The RETINA project\cite{Lazzari:2801062} is a pioneering effort aimed at advancing technologies for real-time track reconstruction in high-energy physics experiments. It leverages FPGAs to create a computing architecture inspired by the early stages of mammalian visual processing. Following its initial research phase, the project has developed a demonstrator that reconstructs data from the LHCb's VELO detector for Run-3 operations.

RETINA's focus on FPGA-based computing allows for rapid, high-throughput data processing. This is achieved through a parallel processing pipeline that reconstructs particle hits from raw pixel data in a manner similar to mammalian brain pattern recognition. The project's design aligns with the concept of ``edge-computing," where the computational load is distributed across hardware components closer to the source of the data, reducing latency and easing the burden on central processing resources.

One of the key components of the RETINA project is the ``Artificial Retina"\cite{Ristori:2000vg}, a pattern-matching algorithm inspired by biological visual processing. This algorithm plays a critical role in the reconstruction of downstream tracks, which is challenging due to the high computational load and complexity of traditional HLT processes. The Artificial Retina is designed to recognize specific track patterns by comparing them to stored reference patterns, providing a non-binary response that varies depending on the distance from the stored patterns.

The process begins with a $2$D cluster-finding algorithm implemented in the 52 readout FPGAs of the VELO, for a total of 104 parallel channels. Each channel further subdivides clusters among multiple parallel sub-channels. The internal pipelined architecture enables swift execution of specific operations ``on the fly", without causing delays in data acquisition, even at high hit rates.

The RETINA project’s goal is to enhance the overall processing power of LHCb by providing a real-time tracking device that operates alongside the traditional data acquisition system. This requires low latency and high throughput, both of which are achievable with FPGAs. The Downstream Tracker, a component of RETINA, receives hits from the SciFi detector in parallel to the main stream of data delivered to HLT$1$, allowing for more efficient reconstruction of downstream tracks.

The integration of the RETINA-based Downstream Tracker into LHCb's data acquisition system requires special considerations\cite{Lazzari:2888549}. The most effective way to implement this integration is by installing the Tracking Boards within unused slots inside the Event Builder (EB) server nodes. The distribution network is established through high-speed optical links, allowing seamless data transmission between different components.

The RETINA project has achieved significant milestones, demonstrating 40 MHz event throughput with sub-microsecond latencies when applied to a generic 6-layer tracking system. The project has transitioned from the R\&D phase to the implementation of a realistic prototype for reconstructing tracks within the VELO acceptance under Run-3 conditions. Data from the VELO detector are sent in a parasitic stream to a facility called the Coprocessor Testbed, where innovative prototypes are being tested under realistic data acquisition conditions. This facility allows for further optimization and validation of the RETINA project before full deployment in the LHCb experiment.

Overall, the RETINA project represents a significant advancement in real-time track reconstruction, providing a flexible and efficient FPGA-based solution for high-energy physics experiments like LHCb. Its approach to embedded reconstruction and edge-computing paves the way for more efficient data processing and analysis, supporting the future of high-energy physics research.

\section{The Clustering}

The ``Artificial Retina" algorithm, crucial for track reconstruction in the LHCb experiment, relies on accurate hit coordinates on the VELO detector. However, the VELO does not inherently produce these coordinates but instead provides information on which pixels are active within an event. This subtle difference can have a significant impact on tracking performance.

When a particle strikes a detector layer, it can activate multiple adjacent pixels. If a tracking algorithm treats each of these pixels as a separate hit, the complexity of resolving hit combinations increases exponentially. This leads to the potential for reconstructing multiple tracks from different pixels activated by a single particle hit. The tracking algorithm can remove these redundant tracks, but this requires additional processing time and resources.

To avoid these issues, contiguous active pixels are grouped into clusters before tracking. The process of finding clusters on a $2$D detector at LHC rates is challenging, and there is limited literature on the topic. In LHCb's HLT$1$, clustering was originally intended to use around 17\% of its computing resources. However, since the ``Artificial Retina" operates before the trigger, it requires an independent clustering process. Implementing this clustering task on FPGA reduces the hardware resources available for tracking, but it is possible to execute the clustering on the VELO's readout boards. This solution dedicates all the Tracking Boards' resources to tracking and reduces the load on the trigger farm by providing direct clusters to HLT.

To address these challenges, a FPGA-based clustering algorithm was developed capable of operating at a \SI{30}{\mega\hertz} event rate in real time, using a relatively modest amount of hardware resources. The typical cluster sizes for charged particles crossing VELO layers are small, with 96\% containing between one and four pixels. Larger clusters typically result from merged hits or secondary emissions (e.g., $\delta$-rays). This property suggests that the FPGA clustering algorithm should be optimized for small clusters.

Given that the average number of clusters per event for a VELO sensor is around seven, the overall detector occupancy remains low. Each sensor contains approximately 197,000 pixels, yielding an expected occupancy rate of around 0.125\% \cite{Bediaga:2013tje}. This low occupancy rate implies that a single large matrix for the entire sensor may not be the most efficient approach. Instead, smaller matrices that dynamically map different sensor areas are preferred. This strategy allows for a reduction in hardware resource usage while maintaining effective clustering capabilities. The developed FPGA-based clustering algorithm is designed with these considerations in mind, providing a robust and efficient solution for the VELO detector's clustering needs.

\subsection*{The Algorithm}
This algorithm has specific features designed for the VELO detector but can be generalized for other silicon pixel detectors. One of the key concepts in this clustering approach is the use of "SuperPixels" (SPs), which are aggregated groups of 4×2 pixels. Each SP contains information about which of the eight pixels was active, its coordinates, and whether any neighboring SPs are active. If none of the surrounding SPs has active pixels, the current SP is flagged as isolated. This information is leveraged by both HLT and FPGA algorithms to optimize the clustering process , allowing for faster algorithms for isolated SPs. Notably, about 53\% of SPs are isolated.
These SPs contain detailed information about pixel activity within each block and are grouped into 256-bit packets called frames, each containing eight SPs. To manage the flow of data and provide context for clustering, each packet contains specific signals, including Start Of Package (SOP), End Of Package (EOP), and Data Valid. The SOP signal indicates the beginning of a new event, while the EOP signal marks the end, with the Data Valid signal providing information about the validity of the SP data.

The Clustering algorithm operates in two different ways depending on the isolation flag of the SPs.
\subsubsection{Clustering resolved entirely with LUTs for isolated SPs}
An isolated SPs can be processed quickly with a look-up table (LUT). Isolated SPs can only have a limited number of possible configurations due to their smaller size and lack of neighboring activity. The clustering algorithm takes advantage of this by using a LUT that contains pre-calculated cluster centers for each possible SP configuration. With $2^8$ possible configurations, this LUT-based approach provides extremely fast processing with minimal logic resource usage.

When an isolated SP is identified, the algorithm looks up its pixel configuration in the LUT and retrieves the corresponding cluster center-of-mass. Since isolated SPs can contain up to two clusters, the LUT also provides this information. This mechanism allows for efficient cluster resolution without extensive computations. Given the high probability of having isolated SPs, this approach significantly reduces the computational load.

\subsubsection{Clustering algorithm for Non-Isolated SPs}
For non-isolated SPs, the clustering algorithm uses a more complex approach, involving the parallel processing of multiple neighbor SPs. To manage this complexity, the process employs a two-stage algorithm\cite{Lazzari:2813167}:
\begin{enumerate}
    \item A matrix filling mechanism for identifying cluster candidates
    \item Cluster Regognition through Pattern Matching
\end{enumerate}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/matrix_filling_clustering.png}
    \caption{Matrix filling example for non isolated pixel}
    \label{fig:matrix_filling_clustering}
\end{figure}

\paragraph{The matrix-filling mechanism}
This approach uses matrices of 3 rows and 3 columns, creating a 12x6 pixel structure that can contain multiple SPs. The matrices are not static; they are dynamically filled based on the incoming SPs.

The first SP in a given event defines the center of a blank matrix, establishing the physical coordinates of that matrix within the VELO detector. Subsequent SPs fill the neighboring cells if they share common coordinates. If an SP does not match any existing matrix, it moves forward in a chain of matrices, eventually filling the center of a new blank matrix. This process is repeated until the EOP word is read.

Figure \ref{fig:matrix_filling_clustering} illustrates this matrix-filling mechanism, demonstrating how SPs find their appropriate place in the matrix structure.

\paragraph{Cluster Regognition through Pattern Matching}
As soon as the EOP word is read, the algorithm uses a pattern-matching technique to identify clusters in a complete parallel way. Each pixel within the matrix checks for specific patterns that signify cluster presence. The most common pattern involves an ``L" shape, where an inactive sequence of pixels surrounds a cluster, providing a natural boundary. A second pattern, involving a diagonal arrangement of two active pixels, is used to identify clusters that might be missed by the first pattern. The combination of these patterns allows for high cluster recognition efficiency.

Once a pattern is matched, the cluster candidate is recognized within the 3x3 grid, as shown in Figure \ref{fig:cluster_recognition}. The center-of-mass of the cluster candidate is then calculated using a LUT with 512 entries, which provides the cluster's absolute position. This position is determined based on the matrix's location within the VELO detector and the cluster candidate's position within the matrix.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/cluster_recognition.png}
    \caption{Pixel patterns associated to clusters}
    \label{fig:cluster_recognition}
\end{figure}

\section{The VELO Cluster Counters}\label{sec:velo_counters}
%\textit{Definition of the selection regions, implementation of the counters on FPGA, counters per bxid, counters per bxtype}
Cluster counters on the VELO are critical components for generating the primitives used to estimate the experimental parameters studied in this thesis: luminosity, beamline position, and VELO position. 

The cluster counters are designed to count the number of clusters within specific regions of the VELO detector, providing fundamental data for further analysis. The firmware, following the clustering process, counts clusters in eight distinct selection regions within each of the 26 VELO stations, resulting in a total of 208 (8 × 26) individual counters. In Figure \ref{fig:VELO-counters} we report a scheme of a VELO station with the position of the selection regions highlighted in red and black. Each one of these selection regions is located on a different sensor of the VELO and has dimension of 20 × 110 pixels. Specifically, the inner counters (red) cover the region between row 80 and row 100, column 620 and 730, while the outer counters (black) cover a region from row 20 to row 40 and from column 29 to column 139. This segmentation enables a detailed view of cluster distributions across the detector, offering valuable insights for the analysis of experimental events.% The choice of these selection region was done empirically, it is not assured that these region are the most sensible for the quantities to measure in this thesis. 


\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/counters.png}
    \caption{A station of the VELO detector with the selection regions highlighted}
    \label{fig:VELO-counters}
\end{figure}

The cluster counters are configured to be read at an arbitrary frequency, allowing flexibility in data collection. In the current firmware implementation, these counters are read every three seconds, integrating information from approximately 120 million events. 
Each counter tracks the average number of clusters per event, ensuring that the total count is normalized by the number of events within the integration window. Furthermore, a running average is performed to guarantee continuity with consecutive readings of mean cluster counts.

To improve the accuracy of measurements and reduce noise from background events, the cluster counters differentiate between four types of bunch crossings: bunch-bunch (bb), bunch-empty (be), empty-bunch (eb), and empty-empty (ee). This distinction is important because it allows for the separation of proton-proton events (bb) from background events (be, eb, ee), where at least one of the colliding beam is empty, due to an empty bunch (indicated with the letter e). This information is carried out by a 2-bit signal in the TFC data sent by the clustering block firmware. In the TFC data also the bunch crossing ID (BXID) is also carried out.

Each one of the different bunch crossing counters is read and stored, but it is convenient to define already the cluster counter outputs used for the construction of the local primitives as: 
\begin{equation}
    c^i_{AVG} = c^i_{bb} - c^i_{be} - c^i_{eb} + c^i_{ee},
\end{equation}

where $i$ is index indicating the counter, spanning from 1 to 208, while the subfix refers to the bunch crossing type.

The information obtained from the 208 cluster counters forms the basis for building embedded primitives that are central to this thesis. These primitives are used to estimate luminosity, beamline position, and VELO position. The online background subtraction and ability to read cluster counts at a high frequency enable real-time data analysis, facilitating more accurate and timely estimates.

In the following chapters, I will explain how the cluster counters are utilized to create embedded estimators for these key parameters. This process involves integrating the data from the counters, applying appropriate algorithms, and extracting meaningful information for further analysis. 