%!TEX root = ../dissertation.tex

\chapter{Embedded reconstruction of local primitives}
\label{chp:retina}
%\epigraph{We here propose to do just what Copernicus did in attempting to explain the celestial movements. When he found that he could make no further progress by assuming that all the heavenly bodies revolved round the spectator, he reversed the process and tried the experiment of assuming the spectator revolved, while the stars remained at rest.}{Immanuel Kant, \textit{Critique of Pure Reason}}
%\textit{Iniziare la ricostruzione a tempo 0 in maniera embedded durante il readout}

The LHCb collaboration has made of ``real-time analysis" one of its trademarks, being the first HEP experiment to implement a real-time fully software-based trigger with an offline-quality reconstruction level at an average frequency of~\SI{30}{\mega\hertz}. This implies processing a flow of~$\sim~\SI{40}{\tera\bit\per\second}$, which poses a significant computational challenge.
%addressed by an heterogeneous computing system that leverage the parallel computing properties of an array of $\sim~500$~GPUs, as described in Section \ref{sec:rta}.
%However, even if the RTA project showed outstanding results, there are some issue to address. 
%this idea relies mostly on leveraging the power of parallel computing such as the one of GPUs. 
%The ``real-time" reconstruction described in Section \ref{sec:rta} essentially runs the same algorithms that would be used for offline reconstruction, but at a much faster speed. This acceleration relies mostly on leveraging the power of parallel computing in centralised servers such as the farm of GPUs and CPUs implemented in the Event Builder and Event Filter Farm.
At the core level, the complete online reconstruction is resource intensive and time consuming, leading to potential bottlenecks in data processing, such as storing data in a buffer awaiting for them to be processed by HLT$2$, as presented in Section \ref{sec:rta}. This can lead to delays in the trigger system, which are avoided by adopting a lighter processing stage in HLT$1$, using a reduced amount of information to alleviate processing load.
However, even this limited preliminary stage requires computationally expensive procedures. For example, HLT$1$ processes a list of detector hits to extract tracks and primary vertices \cite{https://doi.org/10.5281/zenodo.8119731}. This process requires raw data (the list of detector hits) to be read and reassembled to extract meaningful information (the tracks and the PVs). This resources-extensive procedure is made possible in LHCb by using the power of parallel processing of the GPUs mounted in the EB nodes that exploit the Single Instruction on Multiple Data paradigm\cite{5009071} for parallelisation of the event reconstruction and reaching the total throughput of the data stream.
%We could reduce the amount of computation, if we develop algorithms that provide directly the required information directly in the readout of the detector.

Moving towards the future, LHCb is planning to introduce further computing power at an even earlier stage, performing non-trivial reconstruction steps already during the detector readout. This procedure allows to improve the selectivity of its trigger, and allow data taking at higher luminosities. 
Since no event parallelisation is possible at this level, the throughput must necessarily match the LHC collision rate of \SI{30}{\mega\hertz}, and this requires adopting faster technologies based on custom-programmed FPGAs.
In this context, LHCb is developing RETINA, an FPGA-based track reconstruction system capable of \SI{30}{\mega\hertz} throughput, to be deployed in Run~4\cite{Morello:2888549}. 

In this chapter we present the concept of ``embedded reconstruction of local primitives", i.e. the possibility of starting the event reconstruction in an embedded way directly in the readout of the detector, making use of simple statistics computed at high rates. The concept of primitives is not new to LHCb. We can define primitives as object produced from raw data required to produce higher level objects. In a tracking example, the primitives are the \textit{hits} produced from the active channels in the detector (i.e. the raw data), while the high level objects are the final reconstructed tracks. 
The novelty of our approach consists in computing the primitives \textit{directly} during the detector readout. RETINA is a prime example of this ``edge-computing"  approach, showing how a complicated procedure such as track reconstruction can be shifted from a centralised server of GPUs at low throughput to distributed locations closer to the data source, i.e. directly within the readout boards of the detector.
In fact, the first stage of RETINA has already been implemented in the readout firmware of the VELO for the ongoing Run~3. This stage consists in the reconstruction of particle hit coordinates out of the 2D raw detector data (clusterisation). The HLT1 VELO cluster-finding algorithm requires $\sim 17\%$ of the GPUs computing power. The limited resources required by RETINA clustering was already available in the FPGAs mounted in the current VELO readout boards, and therefore it was adopted already in Run~3.
%In fact, the first stage of RETINA is the reconstruction of particle hit coordinates out of the raw detector data. The limited computing power required by this first step is actually already available in the spare resources existing in the FPGAs mounted in the current LHCb readout boards. For this reason, a 2D cluster-finding algorithm has already been implemented in the firmware of the VELO for the ongoing Run~3.
% to determine the coordinates of all hits in the VELO. 
This implementation had made available the full set of all particle hits in the VELO detector for every event, for the first time accessible for potential further uses in undivided form (i.e. not multiplexed over multiple parallel processing streams as it occurs in the EB).

The goal of this thesis in general is to explore what further advantages might be obtained from the real-time accessibility of a flow (unprecedented in HEP) of $\sim4\cdot 10^{10}$ hits/s from a precision detector, such as the VELO. Focusing on the current situation, the applications of high-statistics hit samples can span different categories, like detector diagnostics, alignment, luminosity and luminous region position determination. 
We focused on applications that could be practically implemented with the very modest residual processing power still available within the readout system of LHCb, and without hampering its throughput. This means basing the processing on reasonably simple statistics, such as the simple counting of rates of reconstructed hits in specific VELO regions.

This chapter presents in detail the concept expressed in this introduction: an overview of the FPGA will be given and RETINA will be presented alongside its clustering algorithm, in order to define the ``cluster counters", that in turn are the primitives used to produce the estimators described throughout this thesis.

The purpose is not only to further enhance today's LHCb capabilities, but also to explore the wider potential of a new approach to HEP data processing, that might in future become widely used, in which pre-reconstructed data is available at a very early stage in the data acquisition chain. 
%To address these challenges, this chapter proposes a different approach: extracting meaningful information directly during detector readout to reduce the amount of data that must be processed. By doing this, the subsequent stages receive only the essential relevant information, reducing the need for additional computation. For example, instead of the list of hits that needs to be converted in tracks and PVs, we could directly directly provide HLT$1$ the mean interaction point. This idea is known as ``edge computing", where computational tasks are shifted from centralised servers to locations closer to the data source -- such as near or directly within the detector. While this approach may sacrifice some flexibility, it can dramatically speed up processing and focus resources on the data that truly matters.
%This embedded approach helps streamline the reconstruction process, starting directly from the readout of the detector.
%The core idea of this chapter is to minimise the amount of data that must be processed by extracting meaningful information directly during detector readout. This way, subsequent stages receive precisely the information they require, eliminating the need for additional computation.
%This concept is called ``edge computing". This philosophy shifts computational tasks from centralised servers to locations closer to the data source, such as directly within or near the detector. The key principle is to start the reconstruction in an embedded way directly in the readout of the detector. 
%If smarter algorithms could be developed to directly exploit only the relevant information—e.g. luminosity, detector position, or beamline position—it would make the process more efficient by reducing unnecessary computation. 

%With Moore's Law showing signs of saturation, traditional centralised-based processing is becoming less viable for tasks that demand rapid and efficient computation. 

%This chapter explores the concept of ``embedded local primitives", which are raw-like data points that align closely with the specific quantities that need to be measured. The central module for enabling this approach is the Field Programmable Gate Array (FPGA), which allows for rapid computation of these primitives. A prime example of how this technique is already aiding in event reconstruction is the FPGA-based VELO clustering. Developed as the first phase of the "Artificial Retina" tracking architecture, it finds two-dimensional clusters of hits in the VELO detector and produces the primitives at the heart of this study.
%Finally, I will introduce the specific mother of all the primitives used in this thesis:

%By counting clusters in specific regions on the VELO detector, we implemented further local primitives, the cluster counters. The cluster counters can be linearly combined to enable real-time measurements of luminosity, beamline position, and VELO detector position, as it will be presented in the next chapters. By embedding the computation of these quantities directly within the detector hardware, these counters offer a swift and efficient means of processing data on the spot, minimising latency and improving the speed and accuracy of crucial measurements. 

%In fact, the alignment constants are calculated only for a certain subset of the data, under the assumption of stability in time. However, if this stability is not verified, potential biases or systematic uncertainties could be added to the reconstructed events. For example, the parameters concerning VELO alignment represented the major contribution to the systematic uncertainty in the precise determination of the $B_S^0-\overline{B_S} ^0$ oscillation frequency~\cite{b0b0soscillation}.







\section[The Field Programmable Gate Array]{The Field Programmable Gate Array $\bigl($FPGA$\bigr)$}\label{sec:FPGA}

In modern digital systems, traditional interconnected integrated circuits are becoming impractical due to their high cost, low efficiency, and reliability issues. Instead, many complex digital systems are implemented on custom integrated circuits designed for specific applications, known as Application Specific Integrated Circuits (ASICs). While ASICs are efficient for mass production, they lack flexibility, especially in experimental or research settings where design changes and rapid iterations are frequent. This is where Field-Programmable Gate Arrays (FPGAs) become invaluable.

As the ASICs, FPGAs are hardware-accelerators composed of integrated circuits that allow users to configure their internal connections, creating custom logic designs. Unlike CPUs and GPUs, which have fixed architectures and execute sequential instructions from memory, FPGAs consist of a matrix of programmable logic blocks that can be interconnected according to a user-defined design. This design can be constructed by the user with hardware description languages (HDLs), like VHDL and Verilog. This unique characteristic makes FPGAs highly adaptable, enabling rapid prototyping and experimentation without the need for expensive fabrication of custom ASICs.

Furthermore, FPGAs offer unique benefits through their architecture, where multiple tasks can be executed in parallel across different logic blocks. This parallelism significantly increases throughput, allowing for high-speed data processing. Additionally, FPGAs can include Hard Intellectual Property blocks, which are optimised for specific functions like memory or high-speed communication.

The low-latency data handling and parallel processing capabilities of FPGAs are the characteristics that make them an ideal candidate for the computation of embedded local primitives, which is the focus of this thesis.

%FPGAs are the key component of the PCIe card used in the EB, that is mainly configured as a TELL40 to act as a readout board for the sub-detectors.
The LHCb experiment relies extensively on FPGA produced by Altera, being this the key component of the PCIe card used in the EB for the sub-detectors readout. The programmable logic blocks within Altera's FPGAs are called Adaptive Logic Modules (ALMs), that include Lookup Tables (LUTs), full adders, D-type flip-flops, and multiplexers. These components can be configured to perform various digital logic functions, arithmetic operations, and data storage. The number of ALMs in an FPGA determines its processing capacity, with the devices used for the purposes of this thesis (Arria-10) containing 427200 ALMs.




\section{RETINA}
%In the context of advancing technologies for real-time track reconstruction, the INFN-RETINA project has emerged with a focus on implementing an FPGA-based computing architecture inspired by mammalian brain early image reconstruction processes. Progressing beyond its initial research phase, for Run3 operations the project has now developed a demonstrator on real hardware for reconstructing data from the VELO. 
%The initial phase involving the reconstruction of particle hits out of the raw pixel data, is performed by means of a 2D cluster-finding algorithm implemented in the 52 readout FPGAs of the VELO, for a total of 104 parallel channels. Each channel further subdivides clusters among multiple parallel sub-channels. Utilizing a pipelined internal architecture, certain operations are executed swiftly "on the fly" for all hits detected in each event, without causing delays in data acquisition or requiring extensive computational resources, provided the firmware is appropriately designed. Notably, this efficiency is maintained despite an average hit rate of approximately $4 \cdot 10^{10}$/s, which may appear prohibitive for exhaustive processing.\\

The RETINA project\cite{Lazzari:2801062} is an initiative aimed at advancing technologies for real-time track reconstruction from raw hits data in high-energy physics experiments. This project leverages FPGAs in order to build a computing architecture with rapid, high-throughput data processing using a parallel processing pipeline inspired by mammalian brain pattern recognition. The project's design aligns with the concept of ``edge-computing", where the computational load is distributed across hardware components closer to the source of the data, reducing latency and easing the burden on central processing resources.
Within the RTA working group, RETINA’s goal is to enhance the overall processing power of LHCb by providing a real-time tracking device that operates alongside the traditional data acquisition system. This requires low latency and high throughput, both of which are achievable with FPGAs. 


One of the key components of the RETINA project is the ``Artificial Retina"\cite{Ristori:2000vg}, a pattern-matching architecture inspired by biological visual processing. This architecture plays a critical role in the reconstruction of downstream tracks at LHCb, which is challenging 
%due to the high computational load and complexity 
for traditional HLT processes. The Artificial Retina is designed to recognise specific track patterns by comparing them to stored reference patterns, providing a non-binary response that varies depending on the difference from the stored patterns. The working principle of the Artificial Retina -- schematized in Figure \ref{fig:retina_algo} -- can be summarised as follows\cite{Ristori:2000vg,Lazzari:2801062, Morello:2888549}:
\begin{enumerate}
\item  the track parameter space is divided in a grid of cells, each one corresponding to a \textit{pattern-track} stored in memory: each pattern-track intersects the layers of the detector in points called \textit{receptors} (in analogy with natural vision);
\item  for each event, the algorithm computes the activation level of each of the cells, by means of a Gaussian weighing function of the euclidean distance of each hit from the receptors;
\item the Artificial Retina identifies tracks as local maxima in the cells space, via a local cluster-finding algorithm, then it interpolates the activation level of the neighbouring cells, allowing to reconstruct tracks at the native resolution;
\end{enumerate}
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/retina-mapping.png}\hfill
    \includegraphics[width=0.4\textwidth]{figures/retina_algo_step2.png}
    \includegraphics[width=0.5\textwidth]{figures/retina_algo_step3.png}
    \caption{Track reconstruction steps with the Retina architecture}
    \label{fig:retina_algo}
\end{figure}
The RETINA project has achieved significant milestones, demonstrating \SI{40}{\mega\hertz} event throughput with sub-microsecond latency when applied to a generic 6-layer tracking system\cite{Abba_2015}. The project has transitioned from the R\&D phase to the implementation of a realistic prototype for reconstructing tracks within the VELO acceptance under Run-3 conditions. Data from the VELO detector are sent in a parasitic stream to a facility called the Coprocessor Testbed, where  prototypes are being tested under realistic data acquisition conditions. This facility allows for further optimisation and validation of the Artificial Retina before full deployment in the LHCb experiment.
The ``Artificial Retina" architecture relies on accurate hit coordinates on the VELO detector. However, the VELO does not inherently produce these coordinates but instead provides information on which pixels are active within an event. For this reason, the track reconstruction process begins with a $2$D cluster-finding algorithm implemented directly in the 52 readout FPGAs of the two VELO modules.

%Within the RTA working group, RETINA’s goal is to enhance the overall processing power of LHCb by providing a real-time tracking device that operates alongside the traditional data acquisition system. This requires low latency and high throughput, both of which are achievable with FPGAs. The Downstream Tracker, a component of RETINA, receives hits from the SciFi detector in parallel to the main stream of data delivered to HLT$1$, allowing for more efficient reconstruction of downstream tracks.

%The integration of the RETINA-based Downstream Tracker into LHCb's data acquisition system requires special considerations\cite{Lazzari:2888549}. The most effective way to implement this integration is by installing the Tracking Boards within unused slots inside the Event Builder (EB) server nodes. The distribution network is established through high-speed optical links, allowing seamless data transmission between different components.



%Overall, the RETINA project represents a significant advancement in real-time track reconstruction, providing a flexible and efficient FPGA-based solution for high-energy physics experiments like LHCb. Its approach to embedded reconstruction and edge-computing paves the way for more efficient data processing and analysis, supporting the future of high-energy physics research.

\section{The Clustering}


%This subtle difference can have a significant impact on tracking performance.

When a particle deposits charge in a detector layer, it can activate multiple adjacent pixels. If a tracking algorithm treats each of these pixels as a separate hit, the complexity of resolving hit combinations increases exponentially. This leads to the potential for reconstructing multiple tracks from different pixels activated by a single particle hit. The tracking algorithm can remove these redundant tracks, but this requires additional processing time and resources.

To avoid these issues, contiguous active pixels are grouped into clusters before tracking. The process of finding clusters on a $2$D detector at LHC rates is challenging, and there is limited literature on the topic. In LHCb HLT$1$, clustering was originally intended to use around 17\% of the available computing resources. However, since the ``Artificial Retina" operates before the trigger, it requires an independent clustering process. In order to avoid reducing the FPGA resources available for tracking, the $2$D cluster-finding algorithm is implemented directly in the 52 readout FPGAs of the two VELO modules. Each TELL40 board receive data from four sensor tiles, and processes them in two separate parallel streams, for a total of 104 parallel channels. Once VELO hits are reconstructed by the clustering firmware block, they can be used in a parallel stream in respect to the main data stream without affecting the performances of the data processing firmware. The internal pipelined structure of the data stream allows to perform ``on the fly” further processing of the data, without introducing any buffering stages while running at full speed i.e. at the bunch-crossing rate of \SI{40}{\mega\hertz}, and using a minimal amount of both logical and memory resources.%This solution dedicates all the Tracking Boards resources to tracking and reduces the load on the trigger farm by directly providing clusters to HLT.

To undertake this task, a FPGA-based clustering algorithm was developed, capable of operating at a \SI{30}{\mega\hertz} event rate in real time, using a relatively modest amount of hardware resources. The typical cluster sizes for charged particles crossing VELO layers are small, with 96\% containing between one and four pixels. 
%Larger clusters typically result from merged hits or secondary emissions (e.g., $\delta$-rays). This property suggests that the FPGA clustering algorithm should be optimised for small clusters.

Given that the average number of clusters per event in a VELO sensor is around seven, the overall detector occupancy remains low. Each sensor contains approximately 197,000 pixels, yielding an expected occupancy rate of around 0.125\%\cite{Bediaga:2013tje}. This low occupancy rate implies that a single large matrix for the entire sensor may not be the most efficient approach. Instead, smaller matrices that dynamically map different sensor areas are preferred. This strategy allows for a reduction in hardware resource usage while maintaining effective clustering capabilities. 
%The FPGA-based clustering algorithm was therefore designed with these considerations in mind, providing a robust and efficient solution for the VELO detector's clustering needs.
The clustering algorithm that is currently operating during Run~3 operation is presented in more detail in the following paragraph.

\subsection*{The Algorithm}
This clustering algorithm has specific features designed for the VELO detector but can be generalised for other silicon pixel detectors. One of the key concepts in this clustering approach is the use of ``SuperPixels'' (SPs), which are the readout format of the VELO, corresponding to aggregated groups of 4~×~2 pixels. Each SP contains information about the activity of the eight pixels, the coordinates of active pixels, and whether any neighbouring SPs are active. If none of the surrounding SPs has active pixels, the current SP is flagged as isolated. This information is leveraged by both HLT and FPGA algorithms to optimise the clustering process, allowing for faster algorithms for isolated SPs. Notably, about 53\% SPs are isolated.
%These SPs contain detailed information about pixel activity within each block and are grouped into 256-bit packets called frames, each containing eight SPs. To manage the flow of data and provide context for clustering, each packet contains specific signals, including Start Of Package (SOP), End Of Package (EOP), and Data Valid. The SOP signal indicates the beginning of a new event, while the EOP signal marks the end, with the Data Valid signal providing information about the validity of the SP data.
The Clustering algorithm operates in two different ways depending on the isolation flag of the SPs.
\subsubsection{Clustering resolved entirely with LUTs for isolated SPs}
Isolated SPs only have a limited number of possible configurations due to their smaller size and lack of neighbouring activity. The clustering algorithm leverages this by using a LUT that contains pre-calculated cluster centres for each possible SP configuration. With $2^8$ possible configurations, this LUT-based approach provides extremely fast processing with minimal logic resource usage.

When an isolated SP is identified, the algorithm looks up its pixel configuration in the LUT and retrieves the corresponding cluster centre-of-mass. Since isolated SPs can contain up to two clusters, the LUT also provides this information. This mechanism allows for efficient cluster finding without extensive computations. Given the high probability of having isolated SPs, this approach significantly reduces the total computational load.

\subsubsection{Clustering for Non-Isolated SPs}
For non-isolated SPs, the clustering algorithm uses a more complex approach, involving the parallel processing of multiple neighbour SPs. To manage this complexity, the process employs a two-stage algorithm\cite{Lazzari:2813167}:
\begin{enumerate}
    \item A matrix filling mechanism for identifying cluster candidates
    \item Cluster Recognition through Pattern Matching
\end{enumerate}

\paragraph{The matrix-filling mechanism}
This stage uses a chain of $3\times3$ matrices, creating a 12~×~6 pixel structure that can contain multiple SPs. These matrices are arrnged along a distribution line, in order to be dynamically filled based on the incoming SPs.

The first SP in a given event defines the centre of a blank matrix, establishing the physical coordinates of that matrix within the VELO detector. Subsequent SPs read in the streamline fill the neighbouring cells if the Čebyšëv distance between them is $1$ (i.e. their coordinates differ at most by 1). If a SP does not match any position in the matrix, it moves forward in the streamline, eventually filling the centre of a new blank matrix. This process is repeated until all the SPs are processed.

Figure~\ref{fig:matrix_filling_clustering} illustrates this matrix-filling mechanism, demonstrating how SPs are allocated their appropriate place in the matrix structure.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/matrix_filling_clustering.png}
    \caption{Matrix filling example for non isolated pixel. The horizontal line corresponds to the SPs distribution line, while squares of the same colour corresponds to adjacent SPs. The four matrices displayed correspond to two cases: the two on the left are already filled with some SPs, while the two on the right are uninitialised matrices. In this example, in this clock cycle we read three different SP (which are blue, green and orange coloured for explanatory reasons) in the distribution line. The blue superpixel is adjacent to the SP filling the matrix on the left, therefore it is added to that matrix. Meanwhile, the firmware checks if the green SP fits into the matrix composed of two red SPs, but since it is not adjacent to the SPs in that matrix, it is not inserted, but moves forward in the distribution line. At the same time, the orange pixel fills an empty matrix since at all precedent clock signals it did not find any adjacent pixel in the already filled matrices. At the next clock signal, the firmware checks if the green SP, that was moved forward in the distribution line, fits into the matrix filled with the orange SP. Eventually, at the subsequent clock signal the green SP will fill the right-most blank matrix that is still empty.}
    \label{fig:matrix_filling_clustering}
\end{figure}

\paragraph{Cluster Recognition through Pattern Matching}
As soon as the matrix filling is complete, the algorithm uses a pattern-matching technique to identify clusters in a parallel way. The firmware checks if the pixel positioning within the matrix corresponds to specific patterns that signify cluster presence. The most common pattern involves an ``L" shape, where an inactive sequence of pixels surrounds an active pixel, providing a natural boundary. A second pattern, involving a diagonal arrangement of two active pixels, is used to identify clusters that might be missed by the first pattern. Figure~\ref{fig:cluster_recognition} shows the two patterns just explained. The combination of these patterns allows for high cluster recognition efficiency.

Once a pattern of pixels is matched, the cluster candidate is recognised within the $3\times3$ grid. The centre-of-mass of the cluster candidate is then calculated using a LUT with 512 entries, which provides the cluster absolute position. This position is determined based on the matrix location within the VELO detector and the cluster candidate position within the matrix.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/cluster_recognition.png}
    \caption{Pixel patterns associated to clusters}
    \label{fig:cluster_recognition}
\end{figure}

\section{The VELO Cluster Counters}\label{sec:velo_counters}
%\textit{Definition of the selection regions, implementation of the counters on FPGA, counters per bxid, counters per bxtype}
The cluster counters on the VELO are the building blocks for generating the primitives used to estimate the experimental parameters studied in this thesis: luminosity, luminous region position, and VELO position. 

The cluster counters are designed to count the number of clusters within specific regions of the VELO detector. After the clustering process, the firmware counts clusters in eight distinct regions within each of the 26 VELO stations, resulting in a total of 208 (8~×~26) individual counters. In Figure~\ref{fig:VELO-counters} we report a scheme of a VELO station with the position of the regions highlighted in red and black. Each region is located on a different sensor of the VELO and comprises 20~×~110 pixels. Specifically, the inner counters (red) cover the region between row 80 and row 100 and between column 620 and 730, while the outer counters (black) cover a region from row 20 to row 40 and from column 29 to column 139. 
%This segmentation enables a detailed view of cluster distributions across the detector, offering valuable insights for the analysis of experimental events.% The choice of these selection region was done empirically, it is not assured that these region are the most sensible for the quantities to measure in this thesis. 


\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/counters.png}
    \caption{A station of the VELO detector with the selection regions highlighted}
    \label{fig:VELO-counters}
\end{figure}

The cluster counters are configured to be read at an arbitrary frequency, allowing flexibility in data collection. In the current firmware implementation, these counters are read every three seconds, integrating information from approximately 120 million events, independently from the luminosity. 
The firmware output corresponds to the average number of clusters per event, ensuring that the total count is normalised by the number of events within the integration window. Furthermore, a running average is performed to guarantee continuity with consecutive readings of mean cluster counts.

To improve the measurement accuracy and reduce noise from background events, the cluster counters differentiate between four types of bunch crossings: bunch-bunch (bb), bunch-empty (be), empty-bunch (eb), and empty-empty (ee). This distinction is important because it allows for the separation of proton-proton events (bb) from background events (be, eb, ee), where at least one of the colliding beams is empty, due to an empty bunch (indicated with the letter e). This information is carried by a 2-bit signal in the TFC data sent by the clustering block firmware. The TFC data also carries the bunch crossing ID (BXID), allowing for the realisation of the cluster counters per individual bunch crossing.

Each one of the different bunch crossing counters is read and stored, but it is convenient to define already the cluster counter outputs, used for the construction of the local primitives, as: 
\begin{equation}
    c^i_{AVG} = c^i_{bb} - c^i_{be} - c^i_{eb} + c^i_{ee},
\end{equation}
where $i$ is the counter index, spanning from $1$ to $208$, while the suffix refers to the bunch crossing type.

The information obtained from the 208 cluster counters forms the basis for building the embedded primitives that used in this thesis. These primitives are used to estimate luminosity, luminous region position, and VELO position. The quantities estimated in this thesis serve for various purposes:
\begin{itemize}
    \item This luminosity estimation could provide a feedback to LHC and to other luminosity detectors installed at LHCb
    \item The estimation of the luminous region could provide a correction for the luminosity measurement and accelerate the process of PVs reconstruction by HLT$1$ algorithms
    \item The monitoring of the two VELO halves could help to detect detector misalignments
\end{itemize}
%The online background subtraction and the ability to read cluster counts at a high frequency enable real-time data analysis, facilitating more accurate and timely estimates.

In the following chapters, we will explain how the cluster counters are utilised to create embedded estimators for these key parameters. 
%This process involves integrating the data from the counters, applying appropriate algorithms, and extracting meaningful information for further analysis. 