%!TEX root = ../dissertation.tex

\chapter{Introduction}
\label{chp:intro}
%\epigraph{Oṃ bhūr buvaḥ svaḥ | tat savitur vareṇyaṃ | bhargo devasya dhīmahi | dhiyo yo naḥ pracodayāt}{Gāyatrī mantra}
\epigraph{If I could remember the names of all these particles, I'd be a botanist.}{Enrico Fermi}

Particle physics, as described by the Standard Model (SM), has undergone rigorous validation over the past five decades. The discovery of the Higgs boson marked a significant milestone, proving the SM's predictive prowess and explanatory breadth. However, despite its successes, the SM leaves several fundamental questions unanswered, particularly within the field of flavor physics.

One enduring conundrum pertains to the organization of fermions into three generations, both in the quark and lepton sectors, and the mechanism governing their mass hierarchy. Similarly, the structure of the Cabibbo-Kobayashi-Maskawa (CKM) quark-mixing matrix remains a subject of investigation, alongside inquiries into the origin of the observed matter-antimatter asymmetry and the presence of non-zero neutrino masses.

In response to these lingering mysteries, the high-energy physics (HEP) community has embarked on a quest for New Physics (NP) phenomena that could furnish experimental evidence for theories beyond the Standard Model (BSM). Proposed BSM scenarios encompass SuperSymmetry (SUSY) models, leptoquarks, and extensions to the gauge boson sector, including the W’ and Z’ bosons.

Experimental tests of the SM unfold on two key fronts: the \textit{energy frontier} and the \textit{precision frontier}. At the energy frontier, researchers seek new on-mass-shell particles directly through high-energy collisions, while at the precision frontier indirect evidence of their existence can be asserted studying subtle deviations from SM predictions through precision measurements. Flavor physics, with its unique capacity to detect effects of new particles contributing at loop level, has played a pivotal role in probing the SM's structure.

Measurements of beauty and charm hadron decay rates and $\mathcal{CP}$ violation asymmetries offer opportunities for exploring physics beyond the SM, even at energy scales much higher than those directly involved in the processes. Notably, these observables, measured at the GeV scale, can probe mass scales ranging from $10^3$ to $\SI{1e4}{\tera\eV}$, contingent upon the underlying structure of new interactions. In contrast, direct searches for new states in other experiments at LHC are typically confined to mass scales in the TeV range.

Precision studies of CP violation in elementary processes are crucial for understanding the cosmological matter-antimatter asymmetry. While the SM incorporates CP violation through the CKM matrix, ongoing efforts are dedicated to uncovering additional CP-violating effects that may account for the observed matter dominance in the universe.

\section{The LHCb physics programme}
Given its potential to elucidate BSM effects, flavor phenomenology has emerged as a cornerstone of experimental particle physics. In this vein, the LHCb experiment was conceptualized and optimized to probe $\mathcal{CP}$ violation in b- and c-mesons, along with their decay rates and modes, providing precision measurements in quark-flavor physics for LHC at CERN.
The achievements of the LHCb experiment, leveraging data from Run 1 and Run 2, underscore its pivotal role in advancing the field of flavor physics. Notable milestones include the discovery of rare decay modes, observations of CP violation in the charm sector, and precise measurements of CKM parameters. Additionally, LHCb has contributed to the discovery of exotic hadronic states and the measurement of various production cross sections in electroweak, QCD, and heavy-ion processes.


An intensity approach to particle physics -such as the one of LHCb- demands enormous datasets to improve the sensitivity of measurements. To achieve this, the LHCb experiment is undergoing significant upgrades aimed at increasing data collection. This boost in data will enable tighter constraints on the parameter space of BSM theories. 


With Run-3 just beginning, the experiment is operating at an instantaneous luminosity that is five times higher than Run-2, achieving a level of $\mathcal{L}=\SI{2e33}{\per\centi\meter\squared\per\second}$. This luminosity is similar to what ATLAS and CMS have already achieved in the past. However, the key distinction for LHCb is the significantly higher cross-sections in its acceptance for the processes it focuses on, particularly for b- and c-quark production. At a center-of-mass energy of $\sqrt{s}=\SI{14}{\tera\eV}$, the cross section reach respectively $\sigma_{pp\rightarrow b\bar{b}}\approx\SI{500}{\micro\barn}$ and $\sigma_{pp\rightarrow c\bar{c}}\approx\SI{2000}{\micro\barn}$, allowing LHCb to produce a hige amount of $B$ and $D$ mesons, thus being optimal to perform precision measurements of quark-flavoured quantities.

The LHCb experiment is poised to ready enhance its capabilities thanks to Upgrade I, notably in its trigger and data-acquisition (TDAQ) system, as it will be later described in this thesis. 
The LHCb Upgrade II programme aims to make full use of the capabilities of a forward acceptance detector during the High Luminosity LHC (HL-LHC) operational period, scheduled to start in 2031. Foremost in the physics programme, are the possibilities of the experiment in its core areas of CP violation and rare decays in flavour physics.

LHCb Upgrade II will be installed during Long Shutdown 4 (LS4), with operations beginning in LHC Run 5 which is scheduled to start in 2031. This Upgrade II experiment will operate at instantaneous luminosity of up to \SI{2e34}{\per\centi\meter\squared\per\second}, an order of magnitude above Upgrade I. LHCb will accumulate a data sample corresponding to a minimum of \SI{300}{\per\femto\barn}. New attributes, installed in LS3 and LS4, will enhance the detector’s capabilities to a wider range of physics signatures. An overview of the LHC schedule is reported in Figure \ref{fig:LHC_schedule}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{resources/LHC-long-term-Apr23.png}
    \caption{The LHC long term schedule.}
    \label{fig:LHC_schedule}
\end{figure}

The LHCb Upgrade II is being developed to lead the flavor physics research during the HL-LHC era. In the SM, weak flavor couplings are not fixed, requiring experimental measurements to establish all matrix elements. However, the SM imposes constraints through the unitary nature of the CKM matrix, creating specific relationships among these elements that are commonly depicted as the "unitarity triangle" in the complex plane. If the measured sides and angles of the unitarity triangle don't align with expected results, it could signal new physics beyond the SM.

A particularly interesting aspect of this measurement is the angle $\gamma$, which can be obtained with high precision and carries negligible theoretical uncertainty. Although it currently has a 5-degree uncertainty, LHCb Upgrade II aims to increase this precision by at least an order of magnitude. Another focus area is the $B_s^0$ weak mixing phase, with expectations to achieve a precision of approximately \SI{3}{\milli\radian}, on par with current indirect determinations based on the CKM fit.

The Upgrade II dataset also presents a unique opportunity to investigate new physics through lepton universality tests in semileptonic decays, such as in $b\rightarrow c l^-\bar{\nu}_l$ transitions. The plentiful semileptonic decays allow for precision studies of CP violation in $B^0$ and $B^0_s$ mixing, with sensitivity to CP asymmetries reaching a few parts in $10^{−4}$. This unprecedented sensitivity offers a robust method to detect new physics effects. Similarly, the indirect $\mathcal{CP}$ violation in the charm system is projected to be very small, about $\mathcal{O}(10^{-4})$ in the SM, but the Upgrade II's improved sensitivity ($\mathcal{O}(10^{−5})$) could increase the chances of observing this phenomenon. Additionally, a comprehensive program of direct CP-violation searches in charm is planned, examining both new physics and Standard Model-sensitive modes.

LHCb Upgrade II supports extensive research into rare decays in the charm and strange sectors. It will explore a wide range of $b\rightarrow s ll$ and $b\rightarrow d ll$ transitions, using both muon and electron modes. This unique ability to distinguish among plausible new physics scenarios is a key strength of the upgrade. For example, the ratio of branching fractions $B(B^0\rightarrow \mu^+\mu-)/B(B^0_s\rightarrow \mu^+\mu^-)$ is a critical observable for assessing minimal flavor violation, with an expected precision of 10\% with \SI{300}{\per\femto\barn} of data.

The LHCb Upgrade II's advanced trigger system, now fully software-based, offers flexibility for studies in the forward region. The precision in measuring the weak mixing angle and $W$-mass benefits from the upgrade's vertexing capabilities and larger datasets. This precision also provides an advantage when exploring the Higgs-to-charm coupling and other key Standard Model parameters. The experiment's focus on forward physics further includes dark sector investigations and long-lived particle searches.

The LHCb Upgrade II will continue the experiment's significant impact on hadron spectroscopy, enabling detailed studies on known states and the potential discovery of new ones. Pentaquark multiplets, beauty-containing pentaquarks, and doubly charmed tetraquarks are among the fascinating targets with the enhanced data from this upgrade.



\section{Towards a \textit{real} real-time based paradigm}
The new TDAQ system installed in Upgrade I, designed to process events in real-time, necessitates the adoption of heterogeneous computing solutions. By harnessing diverse computing architectures, such as GPUs and FPGAs, LHCb aims to tackle the formidable computational challenges posed by track reconstruction and event selection at the LHC's unprecedented collision rates.

The computing infrastructure utilized by HEP experiments has traditionally leaned heavily on general-purpose CPUs, which are widely accessible and have historically seen performance gains driven by increasing clock frequencies. However, the slowdown of Moore's law since the early 2000s, coupled with the evolving demands of the physics landscape necessitating higher precision measurements, has prompted a shift towards specialized solutions to tackle the escalating computational demands posed by experiments. Heterogeneous computing, which integrates computing machines with diverse architectures within the same workflow, offers a strategic approach to optimizing performance by assigning tasks to the most appropriate architecture based on their inherent characteristics. This often involves pairing conventional CPUs with specialized co-processors, such as GPUs or FPGAs, to leverage their unique capabilities in accelerating specific computations and enhancing overall efficiency.

In Run 3, LHCb introduces an innovative trigger system, marking the first time that all collision events at the LHC undergo a real-time and offline-level quality reconstruction, operating at an average frequency of \SI{30}{\mega\hertz}. This decision arises from the need for detailed and flexible trigger selections crucial for the physics events targeted by LHCb. Reconstructing events at this frequency, especially tracks, poses a significant computational challenge. To address this, LHCb adopts a heterogeneous computing system. In this setup, the initial trigger level (HLT1) relies entirely on an array of GPUs, leaving CPU resources for the subsequent level (HLT2), where thorough event reconstructions occur at a reduced rate compared to HLT1. 

However, even if these technologies can grant faster operations thanks to higher performances and parallelization, they are trying to perform the same tasks of an offline analysis in a quicker way. Furthermore, much of the computing time is spent to perform low level and repetitive tasks, such as tracking reconstruction and clustering of particle hits on the detector. As LHC moves its schedule to future runs, the collaboration plans to increase the operational luminosity by another order of magnitude compared to Run 3, making it hard for these kind of reconstructions to keep the pace. The reconstruction sequence would be improved if such tasks would be moved to an earlier stage of the data acquisition chain, instead of being performed during the event reconstruc- tion step. This would be indeed extremely advantageous, saving computing power and freeing the CPU farm of redundant and time-consuming tasks. This would allow to make full exploit of CPU time and computing power that can be devoted to higher level tasks in physics selec- tion, where the flexibility of general-purpose CPUs and their associated high-level software environment is actually well employed.
The idea to perform some tasks of the reconstruction sequence at earlier and earlier stages of the data acquisition chain, even before the event-building stage, is not only relevant to enhance the computing performances of the high-level processing farm, but it also constitutes a new paradigm in HEP experiments. Moving for instance the track reconstruction at the readout level would completely change the way events are assembled at the Event Builder, which would handle already reconstructed tracks instead of raw hits on the detector. A new direction in real-time reconstruction has thus established, for which the output data from a detector would no longer be raw data, but they would be well-reconstructed primitives e.g. tracks or reconstructed particle hits.

In the context of advancing technologies for real-time track reconstruction, the INFN-RETINA project has emerged with a focus on implementing an FPGA-based computing architecture inspired by  mammalian brain early image reconstruction processes. Utilizing a pipelined internal architecture, certain operations are executed swiftly "on the fly" for all hits detected in each event, without causing delays in data acquisition or requiring extensive computational resources, provided the firmware is appropriately designed. Notably, this efficiency is maintained despite an average hit rate of approximately $4 \cdot 10^{10}$/s, which may appear prohibitive for exhaustive processing.

As this project already showed the feasibility of such a project, I propose the reconstruction of more primitives, allowing a \textit{real} real-time analysis of some physical quantities. 
The objective of this thesis work is to exploit the real-time accessibility of all hits from a complex detector at LHC, to explore further potentials of real-time FPGA reconstruction: 
\begin{itemize}
\item real-time monitoring of the beam at the LHCb interaction point;
\item estimate the instantaneous luminosity;
\item monitoring of the VELO's operational status and movements.
\end{itemize}

