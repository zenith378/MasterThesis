%!TEX root = ../dissertation.tex
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{#1}}
\fancyhead{} % clear all header fields
\fancyhead[LE]{\chaptername~\thechapter | \leftmark}
\fancyhead[RO]{\thesection~\rightmark}
\fancyfoot{} % clear all footer fields
\fancyfoot[LE,RO]{\thepage}

    
\chapter{Introduction}
\label{chp:intro}
%\epigraph{Oṃ bhūr buvaḥ svaḥ | tat savitur vareṇyaṃ | bhargo devasya dhīmahi | dhiyo yo naḥ pracodayāt}{Gāyatrī mantra}
\epigraph{If I could remember the names of all these particles, I'd be a botanist.}{Enrico Fermi}

Particle physics, as described by the Standard Model (SM), has undergone rigorous validation over the past five decades. The discovery of the Higgs boson\cite{Aad_2012, Chatrchyan_2012} marked a significant milestone, proving the SM's predictive prowess and explanatory breadth. However, despite its successes, the SM leaves several fundamental questions unanswered, particularly within the field of flavor physics.

One enduring conundrum pertains to the organization of fermions into three generations, both in the quark and lepton sectors, and the mechanism governing their mass hierarchy. Similarly, the structure of the Cabibbo-Kobayashi-Maskawa (CKM) quark-mixing matrix remains a subject of investigation, alongside inquiries into the origin of the observed matter-antimatter asymmetry and the presence of non-zero neutrino masses.

In response to these lingering mysteries, the high-energy physics (HEP) community has embarked on a quest for New Physics (NP) phenomena that could furnish experimental evidence for theories beyond the Standard Model (BSM). Proposed BSM scenarios encompass SuperSymmetry (SUSY) models, leptoquarks, and extensions to the gauge boson sector, including the W’ and Z’ bosons.

Experimental tests of the SM unfold on two key fronts: the \textit{energy frontier} and the \textit{precision frontier}. At the energy frontier, researchers seek new on-mass-shell particles directly through high-energy collisions, while at the precision frontier indirect evidence of their existence can be asserted studying subtle deviations from SM predictions through precision measurements. Flavor physics, with its unique capacity to detect effects of new particles contributing at loop level, has played a pivotal role in probing the SM's structure.

Measurements of beauty and charm hadron decay rates and $\mathcal{CP}$ violation asymmetries offer opportunities for exploring physics beyond the SM, even at energy scales much higher than those directly involved in the processes. Notably, these observables, measured at the GeV scale, can probe mass scales ranging from $10^3$ to $\SI{1e4}{\tera\eV}$, contingent upon the underlying structure of new interactions\cite{Isidori_2010}. In contrast, direct searches for new states in other experiments at LHC are typically confined to mass scales in the TeV range.

Precision studies of CP violation in elementary processes are crucial for understanding the cosmological matter-antimatter asymmetry. While the SM incorporates CP violation through the CKM matrix, ongoing efforts are dedicated to uncovering additional CP-violating effects that may account for the observed matter dominance in the universe.

\section{The LHCb physics programme}
Given its potential to elucidate BSM effects, flavor phenomenology has emerged as a cornerstone of experimental particle physics. In this vein, the LHCb experiment was conceptualized and optimized to probe $\mathcal{CP}$ violation in b- and c-mesons, along with their decay rates and modes, providing precision measurements in quark-flavor physics for LHC at CERN.
The achievements of the LHCb experiment, leveraging data from Run 1 and Run 2, underscore its pivotal role in advancing the field of flavor physics. Notable milestones include the discovery of rare decay modes such as $B^0_s\rightarrow\mu^+\mu^-$\cite{PhysRevLett.111.101805}, observations of $\mathcal{CP}$ violation in the charm sector\cite{Maccolini:2022y6}, and precise measurements of the CKM angle $\gamma$\cite{Aaij_2016}. Additionally, LHCb has contributed to the discovery of exotic hadronic states\cite{FANG202266, PhysRevLett.115.072001} and the measurement of various production cross sections in electroweak, QCD, and heavy-ion processes\cite{ZBoson, Raab:2815873, Duan:2826531}.


An intensity approach to particle physics -such as the one of LHCb- demands enormous datasets to improve the sensitivity of measurements. To achieve this, the LHCb experiment is undergoing significant upgrades\cite{CERN-LHCC-2021-012} aimed at increasing data collection. This boost in data will enable tighter constraints on the parameter space of BSM theories. 


With Run-3 just beginning, the experiment is operating at an instantaneous luminosity that is five times higher than Run-2, achieving a level of $\mathcal{L}=\SI{2e33}{\per\centi\meter\squared\per\second}$. This luminosity is similar to what ATLAS and CMS have already achieved in the past. However, the key distinction for LHCb is the significantly higher cross-sections in its acceptance for the processes it focuses on, particularly for b- and c-quark production. At a center-of-mass energy of $\sqrt{s}=\SI{13}{\tera\eV}$, the cross section reach respectively $\sigma_{pp\rightarrow b\bar{b}}\approx\SI{500}{\micro\barn}$ and $\sigma_{pp\rightarrow c\bar{c}}\approx\SI{2000}{\micro\barn}$\cite{bCrossSection, Aaij:2057627}, allowing LHCb to produce a huge amount of $B$ and $D$ mesons, thus being optimal to perform precision measurements of quark-flavoured quantities.

The LHCb experiment is poised to ready enhance its capabilities thanks to Upgrade I\cite{lhcbcollaboration2023lhcb}, notably in its trigger and data-acquisition (TDAQ) system, as it will be later described in this thesis. 
The LHCb Upgrade II programme\cite{CERN-LHCC-2021-012} aims to make full use of the capabilities of a forward acceptance detector during the High Luminosity LHC (HL-LHC) operational period, scheduled to start in 2031. Foremost in the physics programme, are the possibilities of the experiment in its core areas of $\mathcal{CP}$ violation and rare decays in flavour physics.

LHCb Upgrade II (UP2) will be installed during Long Shutdown 4 (LS4), with operations beginning in LHC Run 5 which is scheduled to start in 2031. The experiment will operate at instantaneous luminosity of up to \SI{2e34}{\per\centi\meter\squared\per\second}, an order of magnitude above Upgrade I. LHCb will accumulate a data sample corresponding to a minimum of \SI{300}{\per\femto\barn}\cite{Efthymiopoulos:2319258}. New attributes, installed in LS3 and LS4, will enhance the detector’s capabilities to a wider range of physics signatures. An overview of the LHC schedule is reported in Figure \ref{fig:LHC_schedule}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/LHC-long-term-Apr23.png}
    \caption{The LHC long term schedule.}
    \label{fig:LHC_schedule}
\end{figure}

The LHCb UP2 is being developed to lead the flavor physics research during the HL-LHC era\cite{lhcbcollaboration2019physics}. In the SM, weak flavor couplings are not fixed, requiring experimental measurements to establish all matrix elements. However, the SM imposes constraints through the unitary nature of the CKM matrix, creating specific relationships among these elements that are commonly depicted as the \textit{unitarity triangle} in the complex plane. If the measured sides and angles of the unitarity triangle don't align with expected results, it could signal new physics beyond the SM\cite{PhysRevLett.10.531, 10.1143/PTP.49.652}.

A particularly interesting aspect of this measurement is the angle $\gamma$, which can be obtained with high precision and carries negligible theoretical uncertainty. Although it currently has a 4-degree uncertainty\cite{LHCb:2021dcr}, LHCb Upgrade II aims to increase this precision by at least an order of magnitude. Another focus area is the $B_s^0$ weak mixing phase, with expectations to achieve a precision of approximately \SI{3}{\milli\radian}, on par with current indirect determinations based on the CKM fit.

The UP2 dataset also presents a unique opportunity to investigate new physics through lepton universality tests in semileptonic decays, such as in $b\rightarrow c l^-\bar{\nu}_l$ transitions. The plentiful semileptonic decays allow for precision studies of $\mathcal{CP}$ violation in $B^0-B^0_s$ mixing, with sensitivity to $\mathcal{CP}$ asymmetries reaching a few parts in $10^{−4}$. This unprecedented sensitivity offers a robust method to detect new physics effects. Similarly, the indirect $\mathcal{CP}$ violation in the charm system is projected to be very small, about $\mathcal{O}(10^{-4})$ in the SM, but the UP2's improved sensitivity ($\mathcal{O}(10^{−5})$) could increase the chances of observing this phenomenon. Additionally, a comprehensive program of direct $\mathcal{CP}$-violation searches in charm is planned, examining both new physics and Standard Model-sensitive modes.

The LHCb Upgrade II will continue the experiment's significant impact on hadron spectroscopy, enabling detailed studies on known states and the potential discovery of new ones. Pentaquark multiplets, beauty-containing pentaquarks, and doubly charmed tetraquarks are among the fascinating targets with the enhanced data from this upgrade.



\section{Towards a \textit{real} real-time based paradigm}
The new TDAQ system installed in Upgrade I\cite{lhcbcollaboration2023lhcb}, designed to process events in real-time, necessitates the adoption of heterogeneous computing solutions. By harnessing diverse computing architectures, such as GPUs and FPGAs, LHCb aims to tackle the formidable computational challenges posed by track reconstruction and event selection at the LHC's unprecedented collision rates.

The computing infrastructure utilized by HEP experiments has traditionally leaned heavily on general-purpose CPUs, which are widely accessible and have historically seen performance gains driven by increasing clock frequencies. However, the slowdown of Moore's law since the early 2000s\cite{Tuomi_2002}, coupled with the evolving demands of the physics landscape necessitating higher precision measurements, has prompted a shift towards specialized solutions to tackle the escalating computational demands posed by experiments. Heterogeneous computing, which integrates computing machines with diverse architectures within the same workflow, offers a strategic approach to optimizing performance by assigning tasks to the most appropriate architecture based on their inherent characteristics. This often involves pairing conventional CPUs with specialized co-processors, such as GPUs or FPGAs, to leverage their unique capabilities in accelerating specific computations and enhancing overall efficiency.

In Run 3, LHCb introduces an innovative trigger system, marking the first time that all collision events at the LHC undergo a real-time and offline-level quality reconstruction, operating at an average frequency of \SI{30}{\mega\hertz}. This decision arises from the need for detailed and flexible trigger selections crucial for the physics events targeted by LHCb. Reconstructing events at this frequency, especially tracks, poses a significant computational challenge. To address this, LHCb adopts a heterogeneous computing system. In this setup, the initial trigger level (HLT1) relies entirely on an array of GPUs, leaving CPU resources for the subsequent level (HLT2), where thorough event reconstructions occur at a reduced rate compared to HLT1. 

However, even if these technologies can grant faster operations thanks to higher performances and parallelization, they are trying to perform the same tasks of an offline analysis in a quicker way. Furthermore, much of the computing time is spent to perform low level and repetitive tasks, such as tracking reconstruction and clustering of particle hits on the detector. As LHC moves its schedule to future runs, the collaboration plans to increase the operational luminosity by another order of magnitude compared to Run 3, making it hard for these kind of reconstructions to keep the pace. With UP2, LHCb will reach a pile-up of $\nu\approx 40$ generating more than 2000 tracks, as depicted in the simulation of Figure \ref{fig:velo_pile-up}. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/VELO_pile-up.png}
    \caption{Simulation of hits and tracks generated by different pile-ups. In the upper one the actual situation is depicted, with a pile-up of $\nu\approx 6$, while in the other the one expected for Run5, planned to start after UP2.}
    \label{fig:velo_pile-up}
\end{figure}

The reconstruction sequence would be improved if such tasks would be moved to an earlier stage of the data acquisition chain, instead of being performed during the event reconstruction step. This would be indeed extremely advantageous, saving computing power and freeing the CPU farm of redundant and time-consuming tasks. This would allow to make full exploit of CPU time and computing power that can be devoted to higher level tasks in physics selection, where the flexibility of general-purpose CPUs and their associated high-level software environment is actually well employed.
The idea to perform some tasks of the reconstruction sequence at earlier and earlier stages of the data acquisition chain, even before the event-building stage, is not only relevant to enhance the computing performances of the high-level processing farm, but it also constitutes a new paradigm in HEP experiments. Moving for instance the track reconstruction at the readout level would completely change the way events are assembled at the Event Builder, which would handle already reconstructed tracks instead of raw hits on the detector. A new direction in real-time reconstruction has thus established, for which the output data from a detector would no longer be raw data, but they would be well-reconstructed primitives e.g. tracks or reconstructed particle hits.

In the context of advancing technologies for real-time track reconstruction, the RETINA\cite{refId0} project has emerged with a focus on implementing an FPGA-based computing architecture inspired by  mammalian brain early image reconstruction processes. Utilizing a pipelined internal architecture, certain operations are executed swiftly "on the fly" for all hits detected in each event, without causing delays in data acquisition or requiring extensive computational resources, provided the firmware is appropriately designed. Notably, this efficiency is maintained despite an average hit rate of approximately $4 \cdot 10^{10}$/s, which may appear prohibitive for exhaustive processing.

As RETINA already showed the feasibility of such a project, I propose the reconstruction of more primitives, allowing a \textit{real} real-time analysis of some physical quantities. 
The objective of this thesis work is to exploit the real-time accessibility of all hits from a complex detector at LHC, to explore further potentials of real-time FPGA reconstruction: 
\begin{itemize}
\item real-time monitoring of the beam at the LHCb interaction point;
\item estimate the instantaneous luminosity;
\item monitoring of the VELO's operational status and movements.
\end{itemize}

\section{An urgent motivation of the monitoring tasks}

In the latest years misalignment and drifting movements of the VELO have impacted important measurements, such as the precise determination of the $B_S^0-\overline{B_S} ^0$ oscillation frequency, where the parameters concerning VELO alignment represent the major contribution to the systematic uncertainty\cite{b0b0soscillation}. The need for a faster and more frequent alignment procedure is therefore of maximal urgency.
    
Currently, the alignment and calibration phase of LHCb takes place within the DAQ as part of Real-Time Analysis workflow, as it will be better described in Section \ref{sec:alignment}. Specifically, the alignment of the VELO occurs between HLT1 and HLT2 by minimizing the $\chi^2$ of all tracks with respect to alignment parameters $\alpha$ computed from the residuals of the tracks. The alignment parameters $\alpha$ are then fed back to HLT1 for a new improved track fitting, which in turn refines the estimation of $\alpha$. This iterative procedure is repeated until a convergence in $\alpha$ is achieved\cite{FRUHWIRTH1987444, Frühwirth:803519}. As this process relies on reconstructed tracks, the selected events must be buffered in the Event Filter Farm, resulting in potential delays of hours or even days before alignment is performed. Furthermore, alignment is executed only once per accelerator fill, thus movements of the VELO within the same run are not considered\cite{Dziurda:2640712}. In contrast, the estimator discussed here offers the capability to determine the two VELO halves position with notable precision in real time. This real-time capability contrasts with the current alignment method, providing immediate feedback and the potential to address alignment issues promptly during data collection.

Additionally, the reconstructed beamline positions can also serve as valuable inputs for the reconstruction algorithms of primary vertices (PVs). The current vertex fitting algorithms rely on a hardcoded parameter for the beamline position, followed by an iterative refinement procedure. However, this iterative method is computationally demanding. With the anticipated increase in pile-up to $\nu=35$ under the HL-LHC project, this approach may struggle to keep up and significantly slow down HLT1 performances. Alternatively, supplying the vertex fitting algorithm with a real-time estimation of the beamline could accelerate the process. Integrating my estimator into these algorithms could significantly accelerate the PV finding process by providing updated beamline position inputs.

Furthermore, the current luminosity estimation provided by PLUME suffers from great non-linearity. This deviation from the expected linear behaviour could bias the luminosity measurement. 
Finally, beamline shifts affect every luminosity measurement, independently from the tool used for the estimation. The beamline estimator implemented in this thesis can effectively address these shifts and provide corrections for online luminosity measurements, which are known to depend on the beamline position. By incorporating the estimator into the luminosity measurement process, we can mitigate the impact of beamline shifts and enhance the accuracy of online luminosity measurements.
