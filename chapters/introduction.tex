%!TEX root = ../dissertation.tex
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{#1}}
\fancyhead{} % clear all header fields
\fancyhead[LE]{\chaptername~\thechapter | \leftmark}
\fancyhead[RO]{\thesection~\rightmark}
\fancyfoot{} % clear all footer fields
\fancyfoot[LE,RO]{\thepage}

    
\chapter{Introduction}
\label{chp:intro}
%\epigraph{Oṃ bhūr buvaḥ svaḥ | tat savitur vareṇyaṃ | bhargo devasya dhīmahi | dhiyo yo naḥ pracodayāt}{Gāyatrī mantra}
%\epigraph{If I could remember the names of all these particles, I'd be a botanist.}{Enrico Fermi}
\section{The dawn of the Sandard Model}
Understanding the fundamental constituents of matter and their interactions has been a central pursuit in the history of humanity. The first person who proposed the concept of ``Atomism" was the Greek philosopher Democritus, suggesting in the 6th century BC that all matter is composed of indivisible particles called atoms\cite{laertius1853lives}. 
%Empedocles also imagined fundamental elements and forces of attraction and repulsion allowing the elements to interact. 
This theory made its way in history along Epicurus and Plato, reaching Rome in the 1st century BC with Lucretius explaining them in the \textit{De Rerum Natura}. With the decline of the Roman Empire and the rise of Scholasticism the atomic theory was abandoned for many ages. The 17th century, however saw a resurgence in the atomic theory primarily through the work on the corpuscular nature of light by Newton. From the early philosophical musings, the atomism theory entered the scientific world. The tight bond with the philosophical theory was highlighted in the 19th century by John Dalton. Through his work on stoichiometry, he concluded that each element of nature was composed of a single unique type of particle, naming this fundamental particle \textit{atom}. As the physics entered the 20th century, they discovered that atoms are not, in fact, the fundamental particles of nature, but are conglomerates of even smaller ``corpuscles".

The experimental probe of the first corpuscle of matter was given by Thomson, who first observed the electrons in 1892. The atomic model subsequently was finally completed with the discovery of the constituents of the atomic nucleus: the proton (Rutherford 1919) and the neutron (Chadwick 1932). The observation of the photon by Compton in 1924 completed the picture of the stable matter observed matter in our universe, but a new field just opened: the study of subatomic particles. In this field, not only new constituents of matter were discovered, but also the interactions that govern them. In 1933, Enrico Fermi formulated a theory of beta decays (a precursor of what will be called the weak interaction), while Yukawa in 1934 tried to provide a potential responsible for the strong interaction, that binds together the particles in the nucleus of the atom. 

The mid-20th century saw an explosion of particle discoveries, facilitated by the development of particle accelerators, which is a complex machine that uses electromagnetic fields to propel charged particles -- such as protons or electrons -- to high speeds. These accelerated particles are then made collide with each other or with stationary targets (made of atoms), enabling the interaction between the two at high energy. One of the first results of accelerator experiment was the discovery in 1955 of the antiproton, i.e. a particle with the same mass but with opposite physical charges (such as electric charge). 

Particle accelerators such as colliders, enabled physicists to recreate the high-energy conditions of the early universe, allowing for the discovery of a plethora of new particles -- the ``subnuclear zoo" as called by Robert Oppenheimer -- which eventually lead in the quark model proposed by Gell-Mann and Zweig in 1961. In this new theory,  Gell-Mann and Zweig proposed the quarks as the fundamental elements of some type of particles called \textit{hadrons}. 

The quark model explains how quarks combine to form hadrons: baryons (such as protons and neutrons) are composed of three quarks, while mesons are composed of a quark and an antiquark pair. The antiquark is a particular case of antiparticle,  This model not only provided a systematic way to categorise the myriad of hadrons discovered in particle accelerators but also predicted the existence of new particles, which were subsequently confirmed by experiments. Quarks come in different types, known as ``flavors", and carry fractional electric charges. 

As the quark theory was first developed, only three flavours of quarks were assumed (up, down and strange). However, in 1964 the Glashow-Iliopoulos-Maiani (GIM) Mechanism predicted the existence of a fourth quark (the charm), in order to explain why some transitions result suppressed. Of more importance, the GIM mechanism proposed that quarks are organised in a doublet-symmetry such as (up,down) (charm,strange). This doublet was promptly extended with a third generation (top,beauty) in 1973 by Kobayashi and Maskawa. This extension was needed as the two physicists tried to generalise what was first theorised by Nicola Cabibbo, i.e. that there is a mixing between different flavours of quarks. The Cabibbo-Kobayashi-Maskawa (CKM) Matrix is a 3x3 unitarity matrix that explains how quarks can change from one flavour to another through the weak interaction. The unitarity constraint is usually represented as a triangle in -- the \textit{unitarity triangle} -- in the complex plane.

The weak interaction also describes the interaction of quarks with leptons, another family of elementary particles that do not undergo strong interactions. They are divided in two categories: charged leptons (eletron, muon and tauon) and neutral leptons i.e. the neutrinos. Like the quarks, they also come in three different generations.  

The weak interaction was unified with the electromagnetic force in a theoretical description developed by Glashow, Salam, and Weinberg in the 1970s. The establishment of Quantum Chromodynamics (QCD) for the strong interaction, allowed for the formulation Standard Model (SM). The SM is the physical theory that describes the fundamental interactions between elementary particles and classify them. The interactions are mediated by force-carrier bosons. The electromagnetic force is mediated by the photon, the strong force by the gluon, while weak interaction is mediated by the W and Z bosons. The SM also predicts the existence of the Higgs Boson, a boson whose field permeates all of space, providing mass to the particles that interact with it. A schematics of all the elementary particles described by the SM is depicted in Figure \ref{fig:SM}, with the distinction of bosons on the right and fermions (quarks and leptons) on the left. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/Standard_Model_of_Elementary_Particles_Anti.svg.png}
    \caption{The Standard Model of Particle Physics}
    \label{fig:SM}
\end{figure}


\section{The role of LHCb in Standard Model testing}
Particle physics, as described by the Standard Model, has undergone rigorous validation over the past five decades. The discovery of the Higgs Boson~~\cite{Aad_2012, Chatrchyan_2012} marked a significant milestone, proving the SM predictive prowess and explanatory breadth. However, despite its successes, the SM leaves several fundamental questions unanswered. These include the nature of dark matter, the absence of a quantum description of gravity, and the matter-antimatter asymmetry observed in the universe.

In response to these lingering mysteries, the high-energy physics (HEP) community has embarked on a quest for New Physics (NP) phenomena that could provide experimental evidence for theories beyond the Standard Model (BSM).
Proposed BSM scenarios encompass SuperSymmetry (SUSY) models, leptoquarks, and extensions to the gauge boson sector, including the W’ and Z’ bosons.

Experimental tests of the SM unfold on two key fronts: the \textit{energy frontier} and the \textit{precision frontier}. At the energy frontier, researchers allow the production of new particles directly by increasing the energy available in the centre-of-mass system. At the precision frontier, indirect evidence of the existence can be asserted studying subtle deviations from SM predictions through precision measurements. 

Flavour physics, with its unique capacity to detect effects of new particles contributing at loop level, has played a pivotal role in probing the SM structure. One enduring conundrum pertains to the organisation of fermions into three generations, both in the quark and lepton sectors, and the mechanism governing their mass hierarchy. Similarly, the structure of the CKM quark-mixing matrix remains a subject of investigation, alongside inquiries into the origin of the observed matter-antimatter asymmetry in the universe. In fact, according to the Big Bang theory, the universe should have produced equal amounts of matter and antimatter. However, our observations indicate that the universe is predominantly composed of matter. This asymmetry could have been generated in the violation of fundamental symmetries such as the charge conjugation $\mathcal{C}$ and parity $\mathcal{P}$. The $\mathcal{CP}$ violation suggests therefore that if a particle is replaced with its antiparticle and if its spatial coordinates are inverted, the laws of physics should change. This violation is incorporated into the Standard Model through complex phases in the CKM matrix for quarks and the Pontecorvo-Maki-Nakagawa-Sakata (PMNS) matrix for neutrinos mixing, a process analogous to quark mixing. 


%Experiments have observed $\mathcal{CP}$ violation in various flavours: in strange ($K_0$ oscillations, 1964), in beauty () 
%the probability of an initial state $\mathcal{M}$ decaying into a final state $f$ is different than the probability of a $\mathcal{CP}$ state $\overline{\mathcal{M}}$ of decaying into the final state $\overline{f}$
%and the presence of non-zero neutrino masses. 

 %Proposed BSM scenarios encompass SuperSymmetry (SUSY) models, leptoquarks, and extensions to the gauge boson sector, including the W’ and Z’ bosons.

Measurements of beauty and charm hadron decay rates and $\mathcal{CP}$ violation asymmetries offer opportunities for exploring physics beyond the SM, even at energy scales much higher than those directly involved in the processes. Notably, these observables, measured at the GeV scale, can probe mass scales ranging from $10^3$ to $10^4$ TeV, contingent upon the underlying structure of new interactions~\cite{Isidori_2010}. 

In this context, the LHCb experiment emerged as one of the physics experiment aimed at studying $\mathcal{CP}$ violation and rare decays in heavy hadrons. The LHCb detector is a single-arm forward spectormeter located in one of the collision point at the Large Hadron Collider (LHC) at CERN. In its acceptance, the cross section for b- and c-quark production reach respectively $\sigma_{pp\rightarrow b\bar{b}}\approx\SI{500}{\micro\barn}$ and $\sigma_{pp\rightarrow c\bar{c}}\approx\SI{2000}{\micro\barn}$~\cite{bCrossSection, Aaij:2057627}\footnote{At a center-of-mass energy of $\sqrt{s}=\SI{14}{\tera\eV}$}, allowing LHCb to produce a huge amount of $B$ and $D$ mesons, thus being optimal to perform precision measurements of quark-flavoured quantities.
%In contrast, direct searches for new states in other experiments at LHC are typically confined to mass scales in the TeV range, given 

%Precision studies of $\mathcal{CP}$ violation in elementary processes are crucial for understanding the cosmological matter-antimatter asymmetry. While the SM incorporates $\mathcal{CP}$ violation through the CKM matrix, ongoing efforts are dedicated to uncovering additional $\mathcal{CP}$-violating effects that may account for the observed matter dominance in the universe.

%\section{The LHCb physics programme}
%Given its potential to elucidate BSM effects, flavour phenomenology has emerged as a cornerstone of experimental particle physics. In this vein, the LHCb experiment was conceptualised and optimised to probe $\mathcal{CP}$ violation in b- and c-mesons, along with their decay rates and modes, providing precision measurements in quark-flavor physics for LHC at CERN.
The achievements of the LHCb experiment, leveraging data from Run~1 and Run~2, underscore its pivotal role in advancing the field of flavor physics. Notable milestones include the discovery of rare decay modes such as $B^0_s\rightarrow\mu^+\mu^-$~\cite{PhysRevLett.111.101805}, observations of $\mathcal{CP}$ violation in the charm sector~\cite{Maccolini:2022y6}, and precise measurements of the CKM angle $\gamma$~\cite{Aaij_2016}. Additionally, LHCb has contributed to the discovery of exotic hadronic states~\cite{FANG202266, PhysRevLett.115.072001} and the measurement of various production cross sections in electroweak, QCD, and heavy-ion processes~\cite{ZBoson, Raab:2815873, Duan:2826531}.


%An intensity approach to particle physics -such as the one of LHCb- demands enormous datasets to improve the sensitivity of measurements. To achieve this, the LHCb experiment is undergoing significant upgrades~\cite{CERN-LHCC-2021-012} aimed at increasing data collection. This boost in data will enable tighter constraints on the parameter space of BSM theories. 

%With Run-3, the experiment is operating at an instantaneous luminosity that is five times higher than Run-2, achieving eventually a level of $\mathcal{L}=\SI{2e33}{\per\centi\meter\squared\per\second}$.

%The LHCb experiment has enhanced its capabilities thanks to Upgrade~I~\cite{lhcbcollaboration2023lhcb}, notably in its trigger and data-acquisition (TDAQ) system, as it will be later described in this thesis. 

%The LHCb Upgrade~II (UP2) programme~\cite{CERN-LHCC-2021-012} aims to make full use of the capabilities of a forward acceptance detector during the High Luminosity LHC (HL-LHC) operational period, scheduled to start in 2031. Foremost in the physics programme, are the possibilities of the experiment in its core areas of $\mathcal{CP}$ violation and rare decays in flavour physics.

%LHCb Upgrade II will be installed during Long Shutdown 4 (LS$4$), with operations beginning in LHC Run 5 which is scheduled to start in 2031. The experiment will operate at instantaneous luminosity of up to \SI{2e34}{\per\centi\meter\squared\per\second}, an order of magnitude above Upgrade I. LHCb will accumulate a data sample corresponding to a minimum of \SI{300}{\per\femto\barn}~\cite{Efthymiopoulos:2319258}. New attributes, installed in LS$3$ and LS$4$, will enhance the detector’s capabilities to a wider range of physics signatures. An overview of the LHC schedule is reported in Figure \ref{fig:LHC_schedule}.

%\begin{figure}
%    \centering
%    \includegraphics[width=\textwidth]{figures/LHC-long-term-Apr23.png}
%    \caption{The LHC long term schedule.}
%    \label{fig:LHC_schedule}
%\end{figure}

%The LHCb UP2 is being developed to lead the flavor physics research during the HL-LHC era~\cite{lhcbcollaboration2019physics}. 
%In the SM, weak flavor couplings are not fixed, requiring experimental measurements to establish all matrix elements. However, the SM imposes constraints through the unitary nature of the CKM matrix, creating specific relationships among these elements that are commonly depicted as the \textit{unitarity triangle} in the complex plane. 
Moving towards the future, one of the main goal in the LHCb physics programme is the precise measurements of the unitarity triangle in the complex plane, testing the constraints of unitarity imposed by the SM. If the measured sides and angles of the unitarity triangle do not align with expected results, it could signal new physics BSM~\cite{PhysRevLett.10.531, 10.1143/PTP.49.652}.
A particularly interesting aspect of this measurement is the angle $\gamma$, which can be obtained with high precision and carries negligible theoretical uncertainty. Although it currently has a 4-degree uncertainty~\cite{LHCb:2021dcr}, the collaboration aims to increase this precision by at least an order of magnitude. 

Another focus area is the $B_s^0$ weak mixing phase, with expectations to achieve a precision of approximately \SI{3}{\milli\radian}, on par with current indirect determinations based on the CKM fit.

The LHCb collaboration is also investing lepton universality, i.e. the leptons $l$ interacts with gauge bosons with the same strength, differing only by their masses and lifetimes.
The investigation in new physics through lepton universality tests is carried out in semileptonic decays, such as in $b\rightarrow c l^-\bar{\nu}_l$ transitions. The plentiful semileptonic decays allow for precision studies of $\mathcal{CP}$ violation in $B^0-B^0_s$ mixing, with sensitivity to $\mathcal{CP}$ asymmetries reaching a few parts in $10^{−4}$. Similarly, the indirect $\mathcal{CP}$ violation in the charm system is projected to be very small, about $\mathcal{O}(10^{-4})$ in the SM, but the improved sensitivity of the experiment ($\mathcal{O}(10^{−5})$) could increase the chances of observing this phenomenon. Additionally, a comprehensive program of direct $\mathcal{CP}$ violation searches in charm is planned, examining both new physics and Standard Model-sensitive modes.

%The LHCb Upgrade II will continue the significant impact of the experiment on hadron spectroscopy, enabling detailed studies on known states and the potential discovery of new ones. Pentaquark multiplets, beauty-containing pentaquarks, and doubly charmed tetraquarks are among the fascinating targets with the enhanced data from this upgrade.



\section{Towards a \textit{real} real-time based paradigm}
For the ongoing LHC Run~3, LHCb has introduced an innovative trigger system fully based on software. This marks the first time that a \SI{40}{\mega\hertz} stream of LHC collision events is filtered based on offline-quality event reconstruction performed in real time. The reconstruction This implies processing a flow of~$\sim 40$~Tbits/s, which poses a significant computational challenge. 

The computing infrastructure utilised by HEP experiments has traditionally leaned heavily on general-purpose CPUs, which are widely accessible and have historically seen performance gains driven by increasing clock frequencies. However, the slowdown of Moore's law since the early 2000s~\cite{Tuomi_2002}, coupled with the evolving demands of the physics landscape necessitating higher precision measurements, has prompted a shift towards specialised solutions to tackle the escalating computational demands posed by experiments. Heterogeneous computing, which integrates computing machines with diverse architectures within the same workflow, offers a strategic approach to optimising performance by assigning tasks to the most appropriate architecture based on their inherent characteristics. This often involves pairing conventional CPUs with specialised co-processors, such as GPUs or FPGAs, to leverage their unique capabilities in accelerating specific computations and enhancing overall efficiency.

For the renewed trigger system has decided to pursue the path of heterogeneous computing. The first level leverages the power of an array of $\sim 500$~GPUs operating in parallel on different events. This parallelisation allows to take a first-level decision on events to be kept for further analysis. Events selected at this level are then further processed and selected by a system based on commercial CPU servers. This new paradigm allows at once to greatly reduce the data stream saved to disk, and to enhance flexibility for the selection of events of physics interest. 

%LHCb installed a new TDAQ system in Upgrade I~\cite{lhcbcollaboration2023lhcb}, designed to process events in real-time at a offline quality level.  necessitates the adoption of heterogeneous computing solutions. By harnessing diverse computing architectures, such as GPUs and FPGAs, LHCb aims to tackle the formidable computational challenges posed by track reconstruction and event selection at the unprecedented collision rates of LHC..

%In Run 3, LHCb introduces an innovative trigger system, marking the first time that all collision events at the LHC undergo a real-time and offline-level quality reconstruction, operating at an average frequency of \SI{30}{\mega\hertz}. This decision arises from the need for detailed and flexible trigger selections crucial for the physics events targeted by LHCb. Reconstructing events at this frequency, especially tracks, poses a significant computational challenge. To address this, LHCb adopts a heterogeneous computing system. In this setup, the initial trigger level (HL$T1$) relies entirely on an array of GPUs, leaving CPU resources for the subsequent level (HLT$2$), where thorough event reconstructions occur at a reduced rate compared to HLT$1$. 


However, even if these technologies can grant faster operations thanks to higher performances and parallelisation, much of the computing time is spent performing low level and repetitive tasks, such as tracking reconstruction and clustering of particle hits on the detector. As LHC moves its schedule to future runs, the collaboration plans to increase the operational luminosity by another order of magnitude compared to Run~3, making it hard for these kind of reconstructions to keep the pace. With UP2, LHCb will reach a pile-up of $\nu\approx 40$ generating more than 2000 tracks, as depicted in the simulation of Figure \ref{fig:velo_pile-up}. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/VELO_pile-up.png}
    \caption{Simulation of hits and tracks generated by different pile-ups. In the upper one the actual situation is depicted, with a pile-up of $\nu\approx 6$, while in the other the one expected for Run5, planned to start after UP2.}
    \label{fig:velo_pile-up}
\end{figure}

The reconstruction sequence would be improved if such tasks would be moved to an earlier stage of the data acquisition chain, instead of being performed during the event reconstruction step. This would be indeed extremely advantageous, saving computing power and freeing the CPU farm of redundant and time-consuming tasks. This would allow to make full exploit of CPU time and computing power that can be devoted to higher level tasks in physics selection, where the flexibility of general-purpose CPUs and their associated high-level software environment is actually well employed.
The idea to perform some tasks of the reconstruction sequence at earlier and earlier stages of the data acquisition chain, even before the event-building stage, is not only relevant to enhance the computing performances of the high-level processing farm, but it also constitutes a new paradigm in HEP experiments. Moving for instance the track reconstruction at the readout level would completely change the way events are assembled at the Event Builder, which would handle already reconstructed tracks instead of raw hits on the detector. A new direction in real-time reconstruction has thus established, for which the output data from a detector would no longer be raw data, but they would be well-reconstructed primitives e.g. tracks or reconstructed particle hits.

In the context of advancing technologies for real-time track reconstruction, the RETINA~\cite{refId0} project has emerged with a focus on implementing an FPGA-based computing architecture inspired by  mammalian brain early image reconstruction processes. Utilizing a pipelined internal architecture, certain operations are executed swiftly ``on the fly" for all hits detected in each event, without causing delays in data acquisition or requiring extensive computational resources, provided the firmware is appropriately designed. Notably, this efficiency is maintained despite an average hit rate of approximately $4 \cdot 10^{10}$/s, which may appear prohibitive for exhaustive processing.

As RETINA already showed the feasibility of such a project, I propose the reconstruction of more primitives, allowing a \textit{real} real-time analysis of some physical quantities. 
The objective of this thesis work is to exploit the real-time accessibility of all hits from a complex detector at LHC, i.e. to explore further potentials of real-time FPGA reconstruction: 
\begin{itemize}
\item real-time monitoring of the beam at the LHCb interaction point;
\item estimate the instantaneous luminosity;
\item monitoring of the operational status and movements of the VELO.
\end{itemize}

\section{An urgent motivation of the monitoring tasks}

In the latest years misalignment and drifting movements of the VELO have impacted important measurements, such as the precise determination of the $B_S^0-\overline{B_S} ^0$ oscillation frequency, where the parameters concerning VELO alignment represent the major contribution to the systematic uncertainty~\cite{b0b0soscillation}. The need for a faster and more frequent alignment procedure is therefore of maximal urgency.  
    
Currently, the alignment and calibration phase of LHCb takes place within the DAQ as part of Real-Time Analysis workflow, as it will be better described in Section \ref{sec:alignment}. Specifically, the alignment of the VELO occurs between HLT$1$ and HLT$2$ by minimizing the $\chi^2$ of all tracks with respect to alignment parameters $\alpha$ computed from the residuals of the tracks. The alignment parameters $\alpha$ are then fed back to HLT$1$ for a new improved track fitting, which in turn refines the estimation of $\alpha$. This iterative procedure is repeated until a convergence in $\alpha$ is achieved~\cite{FRUHWIRTH1987444, Frühwirth:803519}. As this process relies on reconstructed tracks, the selected events must be buffered in the Event Filter Farm, resulting in potential delays of hours or even days before alignment is performed. Furthermore, alignment is executed only once per accelerator fill, thus movements of the VELO within the same run are not considered~\cite{Dziurda:2640712}. In contrast, the estimator discussed here offers the capability to determine the two VELO halves position with notable precision in real time. This real-time capability contrasts with the current alignment method, providing immediate feedback and the potential to address alignment issues promptly during data collection.

Additionally, the reconstructed beamline positions can also serve as valuable inputs for the reconstruction algorithms of primary vertices (PVs). The current vertex fitting algorithms rely on a hardcoded parameter for the beamline position, followed by an iterative refinement procedure. However, this iterative method is computationally demanding. With the anticipated increase in pile-up to $\nu=35$ under the HL-LHC project, this approach may struggle to keep up and significantly slow down HLT$1$ performances. Alternatively, supplying the vertex fitting algorithm with a real-time estimation of the beamline could accelerate the process. Integrating my estimator into these algorithms could significantly accelerate the PV finding process by providing updated beamline position inputs.

Furthermore, the current luminosity estimation provided by PLUME suffers from great non-linearity. This deviation from the expected linear behaviour could bias the luminosity measurement. 
Finally, beamline shifts affect every luminosity measurement, independently from the tool used for the estimation. The beamline estimator implemented in this thesis can effectively address these shifts and provide corrections for online luminosity measurements, which are known to depend on the beamline position. By incorporating the estimator into the luminosity measurement process, we can mitigate the impact of beamline shifts and enhance the accuracy of online luminosity measurements.
