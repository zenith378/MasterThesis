%!TEX root = ../dissertation.tex

\chapter{Conclusions}
\label{chp:conclusion}
%\epigraph{“In some sort of crude sense which no vulgarity, no humor, no overstatement can quite extinguish, the physicists have known sin; and this is a knowledge which they cannot lose.”}{Robert Oppenheimer}
The new software-based trigger system that LHCb introduced for the LHC Run~3 allowed at once to greatly reduce the data stream saved to disk and to enhance flexibility for the selection of events of physics interest. The full data stream of \SI{50}{\tera\bit\per\second} is processed with an heterogeneous computing system, leveraging the power of $\sim 500$~GPUs operating in parallel on different events.

As LHCb plans to introduce further computing power at an even earlier stage fur Run~4, a 2D cluster-finding algorithm was developed and it is already operating in the Run~3. This algorithm is implemented at firmware level. It runs on FPGAs and it is used to determine the coordinates of all hits in the VELO. Leveraging the power of FPGAs, the algorithm throughput matches the LHC average collision rate of \SI{30}{\mega\hertz}.

The goal of this thesis work was to explore how we can use the high-rate information obtained by the clustering algorithm, by creating local primitives that can be used to estimate quantities in the ongoing Run~3.
Focusing on applications that could be practically implemented, I found that the simple counting of rates of reconstructed hits in 208 specific VELO regions are reasonable primitives to use for higher level object reconstruction. Appropriately combining the counters, I showed that seven linear estimators of interest can be evaluated ``on the fly": a luminosity estimate, an average position of the luminous region in the transverse plane ($x$ and $y$ coordinates), as well as the average position of the two VELO halves in the two components of the transverse plane.

Regarding luminosity, it was found that each of the implemented counters provide a luminosity estimation and that the best way to combine them is to perform a trimmed mean on the 208 luminosity estimates. Amongst the various online luminosity monitoring tasks, this measurement is the best one available in real-time, with 0.3\% statistical precision. This measurement is currently used by the LHCb online monitoring system as a real-time feedback independently from other luminosity counters of the experiment.

By leveraging the Principal Component Analysis, I found one way of combining the counters to monitor the position of both the luminous region. The implemented estimators performed well both on Monte Carlo and on data, providing a measurement with a resolution of the order of $\SI{5}{\micro\meter}$ by integrating 408000 events at $mu=5.5$, i.e. once every \SI{18}{\milli\second}, corresponding to a frequency $f=\SI{55}{\hertz}$. A faster measurement at $f=\SI{1.1}{\kilo\hertz}$ is possible, with a slightly worse resolution of the order of \SI{20}{\micro\meter}.  

The PCA technique can be used to provide an estimate of the two mechanical halves of the VELO. For these estimators the linearity still holds in the two transverse component, with a resolution spanning from $\SI{6}{\micro\meter}$ to $\SI{11}{\micro\meter}$ for a measurement every \SI{18}{\milli\second} at $\mu=5.5$.

These estimators are currently displayed in the official web page of the LHCb monitoring system, Monet, providing a reconstruction of the two VELO halves and luminous region position in real-time during data taking periods.

It is currently under investigation how this information could be integrated in the raw banks of the events, in order to use it for offline analyses and reduce systematics on VELO movements within a Run.It is also under study how to feed this information into the LHCb online system, accelerating the reconstruction of primary vertices and correcting the detector misalignments in real time.

These results are encouraging on two fronts: on the most specific one, we demonstrated how even a basic information at high statistics can be used to generate meaningful quantities for monitoring purposes at a competitive resolution and fast response. Currently, we count clusters within rectangular VELO regions, each multiplied by a weight derived from PCA. These regions cover only 1\% of the VELO area, due to the fact that we could use only the modest residual processing power available in the ongoing Run~3. Exploiting the entirety of the silicon area could significantly improve the position estimators resolution.  In theory, we could narrow the selection regions down to individual pixel areas, assigning a coefficient to each pixel in the VELO detector. This process could be efficiently executed using specific Kernel functions to apply to the cluster distribution on each VELO sensor. 
%Now we are counting clusters in rectangular selection region multiplied by an appropriate weight derived with PCA. In the limit, we could restrict this area to the single pixel area and multiply each pixel in the VELO detector by an appropriate coefficient. This operation can be performed by developing appropriate Kernel functions to apply on each VELO sensor when counting all the hits on the VELO. 
The computational feasibility of this method is given by the fact that this operation can be directly performed in the FPGA during the readout of the detector, i.e. shortly after the clustering operation is executed. By leveraging all the available information in the pixel detector, the luminous region and VELO position could be provided at full collision rate, instead of the modest frequency $f=\SI{1.1}{\kilo\hertz}$.

%Break-throughs of this kind are necessary for the development of the experiment. As LHC will enter his high-luminosity era, we need to

On the most general front, this thesis explored the potential of a new approach to HEP data processing, by testing it on quantities that have been actually measured with the available resources. We demonstrated how the high-rate statistics produced at low level can give essential information for event reconstruction, locally processing the large data flow of the LHCb vertex detector by using a FPGA-based hardware accelerator. 

The quantities estimated in this thesis (luminosity, luminous region position, and positioning of moveable detector elements) showed very good agreement with offline reconstructed values, at a competitive resolution and with a fast response. Breakthroughs of this kind are essential for future runs; without them, the LHCb experiment will stall in the high-luminosity era of LHC.  These results demonstrated the advantages of high-throughput computing at the very early stages of the data processing using specialised computing devices, thus encouraging further exploitation of such technologies in the near future.



%as much as the high-level reconstruction gives at low-rate, i.e. reducing the statistics to analyse.
% In a future in which pre-reconstructed data is available at a very early stage in the data acquisition chain, this approach may become widely used.

%With the upcoming Run~4 and the consequent increase in luminosity, we are preparing for a comeback of edge-computing as a primary resource to exploit.
