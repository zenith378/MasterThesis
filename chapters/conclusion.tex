%!TEX root = ../dissertation.tex

\chapter{Conclusion}
\label{chp:conclusion}
%\epigraph{“In some sort of crude sense which no vulgarity, no humor, no overstatement can quite extinguish, the physicists have known sin; and this is a knowledge which they cannot lose.”}{Robert Oppenheimer}
The new software-based trigger system that LHCb introduced for the current Run~3 allowed at once to greatly reduce the data stream saved to disk an to enhance flexibility for the selection of events of physics interest. The full data stream of \SI{50}{\tera\byte\per\second} is processed with an heterogeneous computing system, leveraging the power of $\sim 500$~GPUs operating in parallel on different events.

As LHCb plans to introduce further computing power at an even earlier stage fur Run~4, a 2D cluster-finding algorithm is already operating in the current Run~3 firmware in order to determine the coordinates of all hits in the VELO. Leveraging the power of FPGA, the algorithm throughput matches the LHC collision rate of \SI{30}{\mega\hertz}.

The goal of this thesis work was to explore what further advantages might be obtained from the real-time accessibility of a flow (unprecedented in HEP) of $\sim 10^{12}$ hits/s from a precision detector, produced by the FPGA-based cluster algorithm. We decided to focus on applications that could be practically implemented with the very modest residual processing power still available within the current readout system of LHCb, and without hampering its throughput. This means basing the processing on reasonably simple statistics such as the simple counting of rates of reconstructed hits in 208 specific VELO regions. Appropriately combining them, it was shown that seven linear estimators of interest can be evaluated ``on the fly": a luminosity estimate, an average position of the luminous region in the transverse plane (x and y coordinates), as well as the average position of the two VELO halves in the two components of the transverse plane.

Regarding luminosity, it was found that each one of the implemented counters could provide a luminosity estimation and that the best way to combine them was to perform a trimmed mean on the 208 luminosity estimates. Along the various online luminosity monitoring tasks, this measurement currently represents one the best available in real-time, with 1\% precision and 0.5\% in-time stability. 

By leveraging the Principal Component Analysis, we found one way of combining the counters for luminous region and VELO position monitoring. The implemented estimators performed well both on Monte Carlo and on data, providing a measurement every three seconds with a resolution in the order of $\SI{5}{\micro\meter}$. These estimators are currently displayed in the official web-page of the LHCb monitoring system, Monet, providing a reconstruction of the two VELO halves and luminous region position in real-time during data-taking periods. It is currently under investigation how this information could be integrated in the raw banks of the events, in order to use it for offline analyses. Another option would be integrating it into the LHCb online system, accelerating the reconstruction of primary vertices and correcting the detector misalignments in real time.

These results are encouraging on two fronts: on the most specific one, we demonstrated how very much little information at high statistics can provide a way of forming statistics for monitoring purposes. In fact, the current selection regions in which the clusters are counted comprehend only 1\% of the VELO area. Exploiting all the information available in all the silicon area could further enhance the resolution of the position estimators. Now we are counting clusters in rectangular selection region multiplied by an appropriate weight derived with PCA. In the limit, we could restrict this area to a single area and multiply each pixel in the VELO detector by an appropriate coefficient. This operation can be performed by developing appropriate Kernel functions to apply on each VELO sensor when counting all the hits on the VELO. The computational feasibility of this method is given by the fact that this operation can be directly performed in the FPGA during the readout of the detector, i.e. shortly after the clustering operation is executed. By leveraging all the available information in the pixel detector, the luminous region and VELO position could be given for every event, instead of once every N integrated events, or at least with enhanced resolution.
On the most general front, this thesis explored the wider potential of a new approach to HEP data processing, that might in future become widely used, in which pre-reconstructed data is available at a very early stage in the data acquisition chain. We are therefore demonstrating how the high-rate statistics produced at low level can give essential information for the event reconstruction, as much as the high-level reconstruction gives at low-rate, i.e. reducing the statistics to analyse.
With the upcoming Run~4 and the consequent increase in luminosity, we are preparing for a comeback of edge-computing as a primary resource to exploit.