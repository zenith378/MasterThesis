%!TEX root = ../dissertation.tex

\chapter{The LHCb Experiment}
\label{chp:experiment}
%\epigraph{Las dein Auge am Rohr, Sagredo. Was du siehst, ist, dass es keinen Unterschied zwischen Himmel und Erde gibt. Heute ist der 10. Januar 1610. Die Menschheit tr√§gt in ihr Journal ein: Himmel abgeschafft.}{Bertolt Brecht, Leben des Galilei}

%\epigraph{Keep your eye at the telescope, Sagredo. What you see means that there is no difference between Heaven and Earth. Today is the 10th January 1610. Mankind will write in its journal: Heaven abolished}{Bertolt Brecht, \textit{Life of Galileo}}
\section{The Large Hadron Collider}
The Large Hadron Collider (LHC) is a \SI{27}{\kilo\meter} long circular accelerator, positioned at CERN, the European Organization for Nuclear Research, on the Swiss-French border. Constructed approximately 100  metres underground, LHC is the world's largest and most powerful particle accelerator, capable of accelerating protons and heavy ions to the unprecedented energy of \SI{14}{\tera\eV}.

Before reaching the LHC, protons are extracted from hydrogen gas and subjected to a series of acceleration stages, depicted in Figure~\ref{fig:LHC_scheme}~\cite{Lopienska:2800984}. Initially, LINAC4 accelerates hydrogen ions~(H$^-$) to \SI{160}{\mega\eV}. These ions then enter the Proton Synchrotron Booster, which strips the electrons, leaving only the protons and accelerates them to \SI{1.4}{\giga\eV}. From there, the protons are injected into the Proton Synchrotron (PS), reaching \SI{26}{\giga\eV}, and then into the Super Proton Synchrotron (SPS), which boosts them to \SI{450}{\giga\eV}. The SPS then injects the beams of protons into the LHC, where they are accelerated to their final energies by radio-frequency cavities and guided through the accelerator by superconducting magnets.
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/CCC-v2022-large.png}
    \caption{The CERN accelerator complex is a succession of machines, where each machine accelerates a beam of particles to a given energy before injecting the beam into the next one in the chain. This next machine brings the beam to an even higher energy and so on. The LHC is the last element of this chain, in which the beams reach their highest energies.}
    \label{fig:LHC_scheme}
\end{figure}
The accelerator contains over 1200 superconducting dipole magnets, each about 15 metres long, to bend the beams, while 392 quadrupole magnets maintain beam focus. To ensure optimal performance, these magnets are cooled to \SI{1.9}{\kelvin} using superfluid helium. The LHC operates with two beams of protons or heavy ions circulating in opposite directions in separate beam pipes. Proton beams within the LHC are divided into 2808 bunches, each containing approximately $10^{11}$ protons. These bunches are time-spaced by multiples of \SI{25}{\nano\second}, resulting in a bunch-crossing frequency of up to \SI{40}{\mega\hertz}, with an average bunch-crossing rate of approximately \SI{30}{\mega\hertz}. 
The LHC started operating in 2008 and collided the first beams at an energy of $\sqrt{s}=\SI{450}{\giga\eV}$ in 2009. Since then, it has undergone significant upgrades during its operation. The start of the physics research programme was marked by the beginning of Run~1 in 2010 with collisions at a centre-of-mass energy of  $\sqrt{s}=\SI{7}{\tera\eV}$, subsequently upgrading this energy to \SI{8}{\tera\eV} in 2012. After LS1, in 2015 the LHC began Run~2 colliding beams at a ground breaking energy of  $\sqrt{s}=\SI{13}{\tera\eV}$. During Run~3, begun in 2022, the LHC aims at reaching its peak design parameters, including a centre-of-mass energy of $\sqrt{s}=\SI{14}{\tera\eV}$ and an instantaneous luminosity of $\mathcal{L}\approx\SI{2e33}{\per\centi\meter\squared\per\second}$.

The high-energy beams collide at four designated points along the LHC ring, which house the major experiments: ATLAS, CMS, ALICE, and LHCb.
The experiments at the LHC serve different purposes. ATLAS and CMS are general-purpose detectors focusing on high-luminosity collisions to study a wide range of physics phenomena, including the search for new particles, extra dimensions, and dark matter. ALICE, a heavy-ion experiment, explores the quark-gluon plasma, a state of matter present shortly after the Big Bang. LHCb specializes in heavy-quark physics and operates at mid-range luminosity, focusing on flavor physics and $\mathcal{CP}$ violation studies.

\section{The LHCb Detector}\label{sec:detector}

The LHCb detector, depicted in Figure~\ref{fig:lhcb-detector}, is a single-arm forward spectrometer, with a pseudorapidity $\eta$ coverage of 2 to 5. This unique design allows it to study particles containing heavy quarks, primarily bottom ($b$) and charm ($c$), which are typically produced at high pseudorapidity during proton-proton ($pp$) collisions. The LHCb underwent a significant upgrade during LS$2$ to accommodate increased luminosity and handle the elevated event rate associated with the upgraded LHC.
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/UT-upgrade-detector-scaled.jpeg}
    \caption{Layout of LHCb-Upgrade I detector, side-view from the center of LHC ring.}
    \label{fig:lhcb-detector}
\end{figure}
With the upgrade, LHCb will operate at an average bunch-crossing rate of approximately \SI{30}{\mega\hertz}, with an average number of pp collisions per bunch-crossing anticipated to be around $\mu=5.5$. The expected peak value of the instantaneous luminosity is expected to be about \SI{2e33}{\per\centi\meter\squared\per\second}. As it will later explained in Section \ref{sec:lumi_levelling}, the luminosity value exponentially decrease during a Fill, due to beam intensity degradation effects. Unlike the other experiments on LHC, LHCb adopts a system called luminosity levelling to maintain the luminosity at a fixed value.  During Run~3 and Run~4, the experiment aims to collect at least \SI{50}{\per\femto\barn} of data, enabling unprecedented precision in measurements within the flavor sector of the Standard Model.

In order to sustain such high luminosity and to enable more precision measurements, key upgrades include:
\begin{itemize}
\item \textbf{Tracking System}: The tracking system has been renewed, with a new silicon pixel detector, the Vertex Locator (VELO), which measures primary and secondary vertices\footnote{A Primary Vertex (PV) corresponds to the point where initial collision occurs, while a secondary vertex is a point in space where a particle decays into other particles after travelling some distance from the PV.} positions with high precision. Additionally, a new silicon strips upstream tracker (UT) and a scintillating fibers tracker (SciFi) for track fitting were installed.

\item \textbf{Trigger System}: The trigger system has been completely overhauled. In Run~1 and Run~2, LHCb used a Level-zero (L$0$) hardware trigger, which reduced the data acquisition rate from \SI{40}{\mega\hertz} to \SI{1.1}{\mega\hertz}, based on information from the Calorimeter system and Muon stations. This system was removed after Run~2 in favour of a more flexible trigger. The upgraded trigger system now reconstructs the entire event in real-time, enabling high selection efficiency for a wide range of events. The upgraded data acquisition system allows LHCb to process all the data from each collision in real time, allowing to select which events to save based on fully reconstructed decay trees. This system plays a crucial role in ensuring the experiment ability to handle increased data flow and maintain high selection efficiency.
\end{itemize}
In the following sections an overview of each sub-detector will be given.

\subsection[VErtex LOcator]{VErtex LOcator $\bigl($VELO$\bigr)$}\label{sec:velo}
The Vertex Locator (VELO)~\cite{Bediaga:2013tje} is a silicon pixel detector designed to reconstruct primary and secondary decay vertices. In general, a silicon detectors consist of a thin layer of silicon material, typically a few hundred micrometers thick, which is doped to create a depletion region. When charged particles pass through the silicon material, they ionise the atoms along their path, creating electron-hole pairs within the depletion region. The resulting ionisation current can be collected and analysed to determine the passage of a particle. Pixel detectors are a particular case of a  silicon detecotrs, as they implement a matrix of discrete sensing elements (pixels), allowing for the detection to have 2 coordinates, instead of the sole one provided by the other most used silicon detectors, i.e. strip detectors. 
%The spacial resolution of the VELO needs to be better than the typical length of $b$ and $c$ hadrons, ranging from $100$ to \SI{500}{\micro\meter}.  

The VELO consists of 26 stations, which are pairs of retractable modules placed within the LHC vacuum pipe, in order to be as close as possible to the beamline. 
A brief schematics of the geometry is reported in Figure~\ref{fig:velo-geometry}~\cite{LHCbVelo:2019flq}. Figure~\ref{fig_velo-xy} shows the behaviour of the two retractable modules in the fully closed (left) and fully open (right) configuration. During beam ramping, the modules are maintained at a safe position of approximately \SI{3}{\centi\meter} from the beamline (fully open), in order to prevent damage from instabilities of the high-energy beams in the LHC. As soon as stable beams are the declared, the VELO moves to its fully closed configuration. In this case, the innermost VELO sensor is just \SI{5.1}{\milli\meter} from the nominal interaction point, making it the closest detector to the beamline in any HEP experiment at CERN. 

A top view of the 26 stations is visible in Figure~\ref{fig_velo-side}, together with the illustrations of the $z$-extent of the luminous region at $y=0$ and the nominal LHCb acceptance given by the pseudorapidity $\eta$. 

\begin{figure}
    \centering
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{figures/aperture.png}
    \caption{View of a station in the x-y plane. The two modules, VELO A on the right and VELO C on the left, can be opened and closed}\label{fig_velo-xy}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{figures/above_view.png}
    \caption{Top view of all the 52 modules in the x-z plane}\label{fig_velo-side}
    \end{subfigure}
    \caption{Two different schematics of the geometry of the VELO. Figure~\ref{fig_velo-xy} shows a view of the plane perpendicular to the beam pipe. The possible aperture of the two modules is also displayed. Figure~\ref{fig_velo-side} shows a view from the side, where we appreciate each side section of the VELO.}
    \label{fig:velo-geometry}
\end{figure}

Each module contains two tiles, and each tile has two sensors. Each sensor consists of 768~√ó~256 pixels, with a thickness of \SI{200}{\micro\meter}. 
The VELO sensors are read out by VeloPix ASICs, each featuring a 256~√ó~256 matrix of active silicon pixels with dimensions of \SI{55}{\micro\meter} √ó \SI{55}{\micro\meter}. The entire VELO system contains over 40 million pixels. The raw hit resolution varies between $9$ and \SI{15}{\micro\meter}, depending on the angle of the incoming particle. To maintain detection efficiency and account for highly inclined tracks, a 2-pixel wide overlap between sensors within the same tile is implemented. This overlap ensures no gaps in the detection coverage.
%designed to operate at high frequencies (up to \SI{40}{\mega\hertz}) and to withstand intense radiation levels.

At the trigger level, the VELO detects tracks with high impact parameters, which are indicative of heavy-flavored particle decays, helping reducing the minimum bias rate by providing initial track information. Offline, high-resolution vertex reconstruction is key to studying heavy-meson oscillations and $\mathcal{CP}$-dependent time asymmetries.
\subsection{Upstream Tracker}
The UT is a micro-strip silicon detector made of four layers~\cite{LHCb:2014uqj}. It replaced the former TT detector, retired due to irradiation. Each UT sensor is composed of \SI{250}{\micro\meter} thick silicon and a \SI{10}{\micro\meter} metalization layer. The segmentation is finer than the TT, with near-beam sensor pitch of \SI{95}{\micro\meter} and dimension of $98$√ó\SI{49}{\milli\meter\squared}, against the TT sensor pitch of \SI{183}{\micro\meter}. In Figure~\ref{fig:UT} a view of the detector is shown~\cite{ut}. 
The UT is a core part of the LHCb physics case, being required to constrain important decays such as $K_S \rightarrow\pi\pi$ that happen beyond the VELO acceptance. The UT also boosts the momentum resolution and helps identify ghost tracks.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/UT.png}
    \caption{Overview of the UT schematic layout}
    \label{fig:UT}
\end{figure}

\subsection{Magnet}
LHCb exploits the forward region of proton collisions and requires a dipole field with a free aperture of \SI{\pm300}{\milli\radian} horizontally and \SI{\pm250}{\milli\radian} vertically. In order to achieve this, LHCb has a magnet~\cite{LHCb:2000xej} consisting of two coils, both weighing 27 tonnes, mounted inside a 1450 tonne steel frame. Each coil is constructed from 10 layers, wound from almost 3000 metres of aluminium cable. A schematics of the magnet is depicted in Figure~\ref{fig:magnet}.
Tracking detectors in the magnetic field have to provide momentum measurement for charged particles with a precision of about 0.4\% for momenta up to \SI[per-mode=symbol]{200}{\giga\eVperc}. This demands an integrated field of \SI{4}{\tesla\meter} for tracks originating near the primary interaction point.
 The lateral aperture of the magnet is defined by the longitudinal extension of the detectors, placed upstream of the magnet. The two coils are each \SI{7.5}{\meter} long, \SI{4.6}{\meter} wide and \SI{2.5}{\meter} high.
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/lhcb-magnet.png}
    \caption{A schematic of the LHCb magnet (yellow) with the yoke (blue) supporting it}
    \label{fig:magnet}
\end{figure}
The LHCb magnet has a unique feature consisting into the possibility to reverse the polarity of the magnetic field (MagUp or MagDown). This allows a precise control of the charge asymmetries introduced by the detector. Particles hit preferentially one side of the detector, depending on their charges, generating potentially large detection asymmetries. If the data samples collected with the two different polarities have approximately equal size and if the operating conditions are stable enough, charge asymmetries are expected to cancel.

\subsection[Scintillating Fibre Tracker]{Scintillating Fibre Tracker$ \bigl($SciFi$\bigr)$}
The Scintillating Fibres (SciFi)~\cite{scifi} is another tracker sub-detector, that provides enhanced pattern recognition, momentum estimation, and overall tracking efficiency.
The SciFi is located downstream of the dipole magnet and consists of three tracking stations: T$1$, T$2$, and T$3$. Each station comprises four detection planes, arranged to measure coordinates in a cross-pattern, with the orientation of the fibre strips at angles of $\SI{0}{\degree}$, $\SI{+5}{\degree}$ and $\SI{-5}{\degree}$ relative to the vertical axis. These layers are made up of 12 modules, measuring \SI{5}{\meter} by \SI{52}{\centi\meter}, with the innermost modules containing six fibre layers, and the others containing five, due to lower radiation levels. This configuration offers high resolution in the bending plane of the magnetic field and supports efficient pattern recognition for charged particles.

The structural design features vertical staves in the first and last detection planes, while the central two have tilted staves, creating a balanced configuration that enhances detection capabilities and minimises gaps in coverage. Each detector module is constructed with $2.5$-meter-long scintillating fibres, \SI{250}{\micro\meter} in diameter. 
A scheme of the detector geometry is reported in Figure~\ref{fig:scifi}.
%Scintillating fibres are made from a polymer core with $1\%$ by weight of fluorescent dye added to enhance scintillation. The fibres generate optical photons upon interaction with ionising radiation. This process involves depositing energy in the polymer core, which excites the material. Due to the inherent limitations of the base polymer in terms of light yield and relaxation time, the addition of the fluorescent dye improves efficiency by matching energy-level structures to enhance photon production.

%The SciFi uses Silicon Photomultipliers (SiPMs) to read the optical photons. These SiPMs are contained in read-out boxes located at the top and bottom of each detection plane, ensuring efficient collection and processing of the scintillation signals. The SiPMs offer high gain, fast response, and compactness, allowing them to be integrated into the compact design of the SciFi system.

The SciFi is designed for a high hit efficiency, with simulated performance indicating a hit efficiency of over 97\% at the end of its operational lifetime. The raw hit resolution is expected to be approximately \SI{42}{\micro\meter}, providing precise measurements of charged particle trajectories.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/scifi.png}
    \caption{A schematics of the SciFi}
    \label{fig:scifi}
\end{figure}


\subsection{RICH}
At LHCb there are two Ring Imaging Cherenkov (RICH) detectors, RICH1 and RICH2, that enable particle identification~\cite{LHCb:2013urp} across a broad momentum range, from $1$ to \SI[per-mode=symbol]{100}{\giga\eVperc}~\cite{Adinolfi_2013}.

Cherenkov radiation occurs when charged particles travel through a dielectric medium at speeds exceeding the local speed of light (superluminal speed). The angle at which Cherenkov radiation is emitted (Cherenkov angle) is directly related to the refractive index of the medium and the velocity of the particle. Given this relation, the Cherenkov angle $\theta_c$ can be used to infer the mass of a particle when its momentum is known:

\begin{equation}
    \cos\theta_c=\frac{1}{\beta n} = \frac{1}{n}\sqrt{1+\biggl(\frac{m}{p}\biggr)^2}
\end{equation}

where $\beta$ is the speed of the particle relative to the speed of light in natural units ($c=1$), $n$ is the refractive index of the medium, $m$ is the particle mass, and $p$ is its momentum.
This principle allows for the identification of different particles based on the Cherenkov angle. The RICH system uses this concept to separate charged hadrons over a momentum range of $1-\SI[per-mode=symbol]{100}{\giga\eVperc}$.
%, essential for studying hadronic final states and central to LHCb's physics goals, including precise measurements of $\mathcal{CP}$ violation and rare decays of $b$ and $c$ hadrons.

RICH$1$ is located between the VELO and the UT, upstream of the spectrometer magnet. It uses two radiators, aerogel (with refractive index $n=1.03$) and C4F10 (with refractive index $n=1.0014$), allowing discrimination of particles over a momentum range of $1-\SI[per-mode=symbol]{60}{\giga\eVperc}$. RICH$1$ covers an angular acceptance of $25-300$ mrad.

RICH$2$ is downstream of the spectrometer magnet, after the last T-station, covering an angular acceptance from $15$ to \SI{120}{\milli\radian} (non-bending plane) and $15$ to \SI{100}{\milli\radian} (bending plane). It uses CF4 (with refractive index $n=1.0005$) as the radiator, allowing discrimination of particles with momentum ranging from $15$ to $\geq\SI[per-mode=symbol]{100}{\giga\eVperc}$. 
%The $\pi-K$ separation in RICH$2$ is approximately 90\% efficient for momenta up to \SI[per-mode=symbol]{30}{\giga\eVperc}.

The RICH detectors use spherical mirrors to focus Cherenkov light, with secondary flat mirrors to guide the photons onto Hybrid Photon Detectors (HPDs). These HPDs are sensitive to photons in the $200-\SI{600}{\nano\meter}$ wavelength range, and they are placed outside the detector acceptance, reducing material costs and protecting them from the spectrometer's magnetic field.

Arrays of multi-anode photomultiplier tubes (MaPMTs) are used to detect individual Cherenkov photons, with RICH$1$ having a total detection area of $\SI{1.6}{\squared\meter}$ and RICH$2$ having a detection area of $\SI{2.2}{\squared\meter}$. 
%This arrangement ensures efficient collection and processing of Cherenkov light for particle identification.

\begin{figure}
    \centering
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{figures/rich1.jpg}
    \caption{RICH$1$}\label{RICH1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=0.78\linewidth]{figures/rich2.png}
    \caption{RICH$2$}\label{rich2}
    \end{subfigure}
    \caption{Schematics side view of the two RICH detectors.}
    \label{fig:rich}
\end{figure}

\subsection{Calorimeters}
\sloppy
The calorimeter system in the LHCb experiment~\cite{LHCb:2000vji} plays its role in identifying electrons, photons, and hadrons, providing a measurement of their energies and positions. The system comprises the Electromagnetic Calorimeter (ECAL) and an  Hadron Calorimeter (HCAL). Both of them are  placed between the RICH$2$ and the second muon station, with an angular acceptance ranging from \SI{25}{\milli\radian} to \SI{250}{\milli\radian} in the bending plane and to \SI{300}{\milli\radian} in the non-bending plane. 
The ECAL is responsible for measuring the energy and position of electrons and photons. It is built with shashlik calorimeter technology\cite{Badier:293003}, consisting of alternating \SI{4}{\milli\meter}-thick polystyrene scintillating tiles and \SI{2}{\milli\meter} lead sheets. The scintillation light is collected using wavelength-shifting fibers and read out by individual photomultipliers (PMTs) mounted at the back of the tiles. The total thickness of the ECAL is 25 radiation lengths, providing sufficient thickness to contain most electromagnetic showers. The calorimeter structure is segmented into three zones depending on the radial distance from the beamline. The inner section has cells with a lateral dimension of $\SI{40.4}{\milli\meter}$, the middle section has cells with a $\SI{60.6}{\milli\meter}$ lateral dimension, and the outer section comprises cells with a $\SI{121.2}{\milli\meter}$ lateral dimension. The energy resolution for the ECAL is approximately 
 $\sigma_E/E~\text{(GeV)}~=~1\%~\oplus~10\%/\sqrt{E\text{(GeV)}}$.

\sloppy
The HCAL is responsible for measuring the energy and position of hadrons. It consists of \SI{16}{\milli\meter}-thick iron layers alternated with $\SI{2}{\milli\meter}$ scintillating fibres, with a total thickness corresponding to 5.6 nuclear interaction lengths. This design is constrained by the space available inside the LHCb cavern. The energy resolution for the HCAL is approximately $\sigma_E/E~\text{(GeV)}~=~9\%~\oplus~70\%~/\sqrt{E\text{(GeV)}}$. With this poor resolution, combined with the incapability of containing the full hadronic shower, the main function of HCAL was the estimation of hadron energy for the low level trigger L$0$ in Run~1 and Run~2. However, it is still used in some jets analysis, but it will be removed in Upgrade2.

The lateral segmentation of both ECAL and HCAL is shown in Figure~\ref{fig:cal_system}.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/CAL_system.png}
    \caption{Lateral segmentation of the ECAL (left) and the HCAL (right). One quarter of the detector front face is shown.}
    \label{fig:cal_system}
\end{figure}
\subsection{Muon Stations}

The muon detector system primary function is to identify and measure the transverse momentum of muons in order to reconstruct decay channels involving muons, which are vital for many LHCb physics studies.
The LHCb muon detector system~\cite{Alves_2013} consists of four stations (M$2$ to M$5$) placed downstream of the hadronic calorimeter and interleaved with thick iron walls that act as muon filters. In Run~1 and Run~2, there was an additional M$1$ station located in front of the calorimeters, used for transverse momentum measurement for the L$0$ trigger~\cite{muon_upgrade}. The system covers the angular acceptance from \SI{20}{\milli\radian} to \SI{306}{\milli\radian} in the bending plane and from \SI{16}{\milli\radian} to \SI{258}{\milli\radian} in the non-bending plane. Each station comprises two mechanically independent halves, called the A and C sides, which can be horizontally moved to access the beam pipe and detector chambers for installation and maintenance.
A scheme of the muon stations (including M1 no longer part of the detector) is depicted in Figure~\ref{fig:muon}.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/muon.png}
    \caption{A scheme of the muon stations. M$1$ was removed during Upgrade~1.}
    \label{fig:muon}
\end{figure}
The stations are equipped with Multi-Wire Proportional Chambers (MWPCs) for muon detection. These chambers use a gas mixture of argon, carbon dioxide, and tetrafluoromethane (CF4). The iron absorbers between the stations add filtering capacity, ensuring that only muons with momentum greater than \SI[per-mode=symbol]{6}{\giga\eVperc} can traverse the entire system.

Stations M$2$-M$3$ focus on transverse momentum measurements, while M$4$ and M$5$ serve to confirm if a particle has traversed the entire detector system. The system achieves a 95\% detection efficiency, providing robust muon identification. 

The unique feature of the LHCb muon detector system is its segmented structure, with each station divided into four regions (R$1$-R$4$), with cells scaling in the ratio 1:2:4:8 with distance from the beam axis. This segmentation allows the system to manage varying particle rates and optimise detection efficiency. Additionally, the independent halves (A and C sides) offer flexibility during installation and maintenance, enabling quick access to the beam pipe and detector chambers when needed.


\section[Real Time Analysis]{Real Time Analysis $\bigl($RTA$\bigr)$}\label{sec:rta}

With the increase in instantaneous luminosity at the LHCb interaction point, the demand for more accurate measurements has driven the collaboration to completely renew its readout and trigger systems. The upgraded system operates at an average event rate of \SI{30}{\mega\hertz}, processing every event with a fully software-based trigger system that integrates information from all sub-detectors. The need for more efficient trigger systems emerged from simulation studies showing that the trigger sequence used during Run~1 and Run~2 would lead to a loss of efficiency in hadronic B-meson decay channels as the instantaneous luminosity increased~\cite{CERN-LHCC-2011-001}. The first stage of the previous trigger system, L$0$, relied on transverse energy $E_T$ measurements from several sub-detectors. This resulted in high efficiencies for dimuon events but reduced efficiency for fully hadronic decays and for decays including electrons and photons in the final state. Additionally, the increase in the $E_T$ threshold to manage trigger rates with higher luminosities compromised signal efficiency, causing saturation of the trigger yield.

The renewed DAQ and trigger system at LHCb is designed to overcome these limitations~\cite{CERN-LHCC-2018-014}. The readout system can handle high input data flow, on the order of \SI[per-mode=symbol]{50}{\tera\bit\per\second}, while the software-based trigger can process events at an average rate of \SI{30}{\mega\hertz}, mathcing the full LHC crossing rate.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/hidef_RTA_dataflow_widescreen.png}
    \caption{Workflow of the Real Time Analysis paradigm, from the detector readout to analysis production}
    \label{fig:RTA}
\end{figure}

The trigger system now implements a two-stage full software solution, schematised in Figure~\ref{fig:RTA}:
\begin{itemize}
\item HLT$1$: the first stage reduces the data rate by a factor of 30-60, by performing a partial reconstruction. It is executed on Graphics Processing Units (GPUs), installed on Event Builder (EB) nodes.
\item HLT$2$: the second stage, running on a farm of Central Processing Units (CPUs) allows for detailed event reconstruction with quasi-offline precision~\cite{Gazzoni:2670650}.
\end{itemize}
HLT$2$ has a very detailed precision, but it suffers from great slowness. In order to not lose the data, a buffer is implemented between HLT$1$ and HLT$2$, as can be seen in Figure~\ref{fig:RTA}. The events accumulate in this buffer, awaiting to be processed by HLT$2$. Due to this accumulation, the full event is processed hours or even days after the collision happened, while a subset of quantities of events are available for immediate storage 
The buffer serves also the purpose of data source for the Alignment \& Calibration phase of the detector. While the data awaits to be processed by HLT$2$, a subset of them is used to perform this task. Once available, the Alignment \& Calibration information is then back-propagated to all the events in HLT$1$, in order to enhance the resolution of estimated parameters (e.g. PVs), and also forth-propagated to HLT$2$ for the offline-quality level reconstruction.



The DAQ system~\cite{CERN-LHCC-2014-016} is composed of several sub-systems:
\begin{itemize}
\item The Timing and Fast Control (TFC) system is responsible for distributing critical clock, timing, and trigger information to the front-end and readout systems. It synchronises to the LHC master clock, ensuring precise timing across all components. The TFC provides global signals, including the \SI{40}{\mega\hertz} clock, synchronous commands to control event processing, calibration commands for detectors, and electronics configuration distribution from the ECS.

\item The Event Builder (EB) hosts the early stage reconstruction (HLT$1$) which is performed on GPUs mounted on the EB nodes

\item The Event Filter Farm (EFF), hosts the farm of CPUs dedicated to perform the selection at HLT$2$ level.

\item The Experiment Control System (ECS) oversees the entire LHCb experiment, from infrastructure to data acquisition.
\end{itemize}
In Figure~\ref{fig:architecture_readout} a scheme of their interaction is shown.
In Section \ref{sec:HLT1eHLT2} we present a more detailed description of the sub-systems relative to the HLT$1$ and HLT$2$, while the Alignment \& Calibration procedure is explained in Section \ref{sec:alignment}. Finally, in Section \ref{sec:ecs} we provide a description in ECS. 
%In the following subsections, a more detailed description of the sub-systems will be given.

\begin{figure}
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{figures/The-architecture-of-Upgraded-LHCb-readout-system.png}
    \caption{Architecture of the data readout and event builder system.}
    \label{fig:architecture_readout}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
         \includegraphics[width=\linewidth]{figures/RTA_trigger.png}
    \caption{Focus on the EB role and transmission lines within the DAQ workflow.}\label{fig:EB_zoom}
    \end{subfigure}
    \caption{Two views of the DAQ architecture and workflow. Each EB node receive information from a particular sub-detector. These information are exchanged using the EB network, in order to re-assign to each EB node the information of all the sub-detecotr of a specific event. Once this is done, the partial event reconstruction is performed on a set of GPUs, which reduce the throughput. The partial reconstructed event is partially stored in the online servers and forth-propagated to HLT for the second-stage trigger.}\label{fig:general_overview_trigger_RTA}
\end{figure}


\subsection{The HLT$1$ and HLT$2$ subsytems}\label{sec:HLT1eHLT2}
\subsubsection{Event Builder (EB)}
To collect data from all sub-detector channels and assemble them into complete event datasets, a process known as event building is employed. This entails gathering all data fragments corresponding to a single event and consolidating them centrally. This task is handled by a dedicated server farm called the Event-Builder (EB), which comprises 163 interconnected nodes. These nodes are linked to the detector front-end, other EB nodes, the buffer towards HLT$2$ farm, and the other control systems, such as TFC and ECS.

The workflow of the event building is as follows: 
The data from all the sub-detectors in the underground area are transmitted to the EB nodes in the surface through radiation-hard 300-meter-long optical fibres with zero-suppression. Each EB node receives data from a specific sub-detector, and it is responsible for its readout. The initial information at each EB node corresponds to the data of a specific sub-detector contained in multiple events. The data of each EB node are shared with other EB nodes to assemble complete event information via the Event Builder Network. Once the Event Builder Network has gained access to all the information regarding the events, the data of each sub-module regarding the same event is send back to a single EB node, which will complete the build in the GPUs installed in the EB node. The Event Builder adopts therefore two levels of parallelisation in order to match the data throughput: on one hand, the EB nodes reconstruct different events simultaneously; on the other, the reconstruction of a single event in each of the EB node leverage the parallel processing of the hardware accelerators mounted inside each EB node.
Once the full event is reconstructed, the data is sent from the EB node to the buffer, awaiting for the full reconstruction by HLT2. 

In the next paragraph we give a description of the EB nodes architecture, in order to better understand how the EB nodes can perform the operations we just described. In the subsequent paragraph we present the operation that HLT$1$ performs within the GPUs mounted in the EB nodes. 

\paragraph{EB node architecture}
An overview of all the modules installed in a single EB network is schematised in Figure \ref{devices_EBnode}. The detector front-end is connected with $\sim10^{4}$ optical fibres to the Readout Boards, also referred to as TELL40 boards. These custom-built PCI Express (PCIe) boards are equipped with an Intel Arria 10 Field Programmable Logic Arrays (FPGA)\footnote{A more detailed description of the FPGA will be given in Section \ref{sec:FPGA}}, boasting one of the largest FPGA sizes available during their development phase. With a PCIe Gen3 interface, these boards can write data to host memory via Direct Memory Access (DMA) at speeds of up to $100$~Gbit/s. Each Readout Board contains logical units, such as the Readout Unit (RU), responsible for transmitting event fragments to other EB nodes through the EB network, with the communication relying on 200~Gbit/s InfiniBand NICs in order to handle the high bandwidth. On the other hand, the Builder Unit (BU) on each node collects fragments corresponding to the same event. Each EB node has the capacity to accommodate up to three accelerator boards for data processing, with GPUs mounted on these slots to execute HLT$1$ operations. The selected events from HLT$1$ are then forwarded to the Event Filter Farm (EFF) via the HLT network, transmitting via 10~Gbit/s Ethernet NICs, which are commonly integrated into server motherboards. 

\begin{figure}
\centering
    \includegraphics[width=0.5\textwidth]{figures/zoom_EB_node.png}
    \caption{Devices installed at each EB node, which comprehend three TELL40s (Readout), three GPUs (Accelerator) and communication ports for the other EB nodes (200G EB net) and EFF (10G HLT net).}\label{devices_EBnode}
\end{figure}

\paragraph{HLT$1$}
HLT$1$, installed directly within the EB, significantly reduces the data rate by a factor of 30-60, enabling the use of a smaller and more cost-effective network between the EB and the EFF. This reduction is crucial considering the raw data immense bandwidth requirement of 40~Tbit/s.

The HLT$1$ stage aims to reject events that do not contain particles of interest while retaining those that do. It uses a subset of the full offline charged particle reconstruction and implements inclusive single or two-track selections to determine the event interest. HLT$1$ is entirely implemented on GPUs in the same servers hosting the Event Builder. The \textit{Allen} application, written in C++ with CUDA extensions, executes the following tasks~\cite{CERN-LHCC-2020-006}:
\begin{itemize}
\item Decoding raw input into the LHCb global coordinate system
\item Clustering detector hits
\item Pattern recognition to identify hit combinations associated with the same particle
\item Fitting track candidates using a Kalman Filter to estimate momentum and other parameters
\item Reconstructing primary and secondary vertices from fitted tracks
\item Making trigger decisions based on track parameters like impact parameter and momentum
\end{itemize}
Once the events are selected they are propagated to the Event Filter Farm. 


\subsubsection{Event Filter Farm (EFF)}

The EFF is exclusively dedicated to running HLT2 and comprises a heterogeneous mixture of CPUs servers with various generations and core counts. 
%Given the asynchronous nature of HLT2 processing, load balancing among these servers is handled as an implementation detail.
While HLT$1$ makes a first-stage cut on events based on long tracks (i.e. created near interaction point) information, HLT$2$ includes allows a more comprehensive event analysis using information from all the sub-detectors. It applies specialised cuts to select events of interest to LHCb. 

HLT$2$ allows also for the reconstruction of downstream tracks, i.e. tracks generated outside the VELO acceptance. This information is crucial, because some neutral long-lived particles, like $K^0_S$ and $\Lambda^0$, are reconstructed as downstream tracks as they decay outside the acceptance of the VELO detector. 


\subsection{The Alignment and Calibration phase}\label{sec:alignment}
LHCb distinguishes between two phases~\cite{Dziurda:2640712}:
\begin{itemize}
\item Alignment focuses on correcting shifts and rotations in the detector's components, ensuring that the data obtained from these detectors are accurate. This process is applied to tracking detectors like VELO, UT, SciFi, and Muon systems, ensuring that their positioning aligns with the expected geometry.
\item Calibration involves fine-tuning the responses of various sub-detectors to ensure accurate measurements. This includes calibrating the RICH, ECAL, and HCAL.
\end{itemize}
Both processes are triggered at the beginning of each LHCb run, and they are fully integrated into the LHCb control system. 
\paragraph{Alignment}
Alignment at LHCb involves minimising the chi-squared $\chi^2$ of all tracks with respect to alignment parameters $\alpha$, which account for translations and rotations of detector elements. The procedure employs the iterative Newton-Raphson method\cite{raphson2011analysis}, where the first and second derivatives of $\chi^2$ with respect to $\alpha$ are calculated to determine the optimal adjustments needed for alignment. The data for this alignment procedure comes from tracks obtained through a Kalman filter used in track reconstruction~\cite{HULSBERGEN2009471}, with the option to add vertex position and mass constraints for increased precision.

The alignment process has two key components~\cite{Saur:20230E}:
\begin{itemize}
\item Analyzer: This part runs the track reconstruction, computes the $\chi^2$ derivatives, and saves them to binary files. It uses multi-threaded reconstruction based on 163 nodes, allowing for efficient data processing.
\item Iterator: This component collects the derivatives from the Analyzer, performs the minimization step, and checks for convergence. If there is a significant difference between the previous and new alignment constants, the updated constants are used in the HLT2 trigger, ensuring that subsequent data collection reflects the corrected alignment.
\end{itemize}

Apart from the track alignments, the RICH mirrors also need to be aligned. This is done by creating histograms of the difference ($\Delta \theta_c$ ) of each detected photon reconstructed Cherenkov angle and its expected one. By fitting the appropriate distribution and correcting for deviations of its mean value from zero in each bin, the RICH mirrors can be aligned. This procedure can take several hours, in contrast to the real-time tracker alignments.
\paragraph{Calibration}
Calibration at LHCb involves ensuring that the detector responses are accurate and consistent. It includes:
\begin{itemize}
\item RICH Calibration: This involves evaluating the refractive index that depends on the RICH temperature and gas pressure;
\item Calorimeter Calibration: ECAL and HCAL calibration focuses on ensuring accurate energy measurements.
\end{itemize}
The calibration process leverages dedicated HLT$1$ trigger lines to select events suitable for calibration and alignment. These events are stored in a buffer until sufficient statistics are accumulated to run the calibration and alignment procedure. This buffering allows HLT$2$ reconstruction to use the most accurate and relevant alignment and calibration constants.


\subsection{Experiment Control System (ECS)}\label{sec:ecs}

The ECS~\cite{GranadoCardoso:2702137} oversees the entire LHCb experiment, from infrastructure to data acquisition. It is a distributed system based on the commercial Supervisory Control And Data Acquisition (SCADA) system WinCC OA, and custom-developed components. The ECS features user interfaces, data archiving, alarm handling, and various device interfaces. Its tree-like control hierarchy, based on a Finite State Machine (FSM), allows commands to be propagated downstream and state information to be propagated upward, as depicted in Figure~\ref{fig:ECS_tree}.
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/ECS_tree.png}
    \caption{ECS schematic structure. The topmost node (the ECS itself) propagates commands to the downstream units (FE, DAQ systems, Trigger, ...) and is interfaced with the LHC system.}
    \label{fig:ECS_tree}
\end{figure}

Of particular interest of this thesis, is that ECS has simultaneous access to all TELL40 boards. As it will later presented, this feature is important in order to combine the information from the different Readout Boards of the VELO, allowing the computations for the statistics of interest in this thesis. To automate the process and ensure regularity, a WinCC control-script is developed inside the VELO WinCC project. The WinCC script allows for the computation of \textit{datapoints} representing the value of the quantity of interest at specific timestamps. The LHCb online monitoring system application, \textit{Monet}, can display them in trending plots or histograms accessible from any browser \cite{Adinolfi:2298467}.

ECS allows to view and manage the entire LHCb experiment from a central location, at any moment. An important component of ECS is the Run Control, which oversees all tasks essential for data collection. The Run Control is in charge of setting up the sub-detectors front-end electronics, voltage system and readout boards, the TFC system, and initialising and configuring all necessary DAQ processes. 
This structure provides centralised control and monitoring, ensuring efficient management of the LHCb experiment.