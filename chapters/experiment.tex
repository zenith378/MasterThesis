%!TEX root = ../dissertation.tex

\chapter{The LHCb Experiment}
\label{chp:experiment}
%\epigraph{Las dein Auge am Rohr, Sagredo. Was du siehst, ist, dass es keinen Unterschied zwischen Himmel und Erde gibt. Heute ist der 10. Januar 1610. Die Menschheit trägt in ihr Journal ein: Himmel abgeschafft.}{Bertolt Brecht, Leben des Galilei}

%\epigraph{Keep your eye at the telescope, Sagredo. What you see means that there is no difference between Heaven and Earth. Today is the 10th January 1610. Mankind will write in its journal: Heaven abolished}{Bertolt Brecht, \textit{Life of Galileo}}
\section{The Large Hadron Collider}
The Large Hadron Collider (LHC) is a \SI{27}{\kilo\meter} long circular accelerator, positioned at CERN, the European Organization for Nuclear Research, on the Swiss-French border. Constructed approximately 100  metres underground, LHC is the world's largest and most powerful particle accelerator, capable of accelerating protons and heavy ions to the unprecedented energy of \SI{14}{\tera\eV}.

Before reaching the LHC, protons are extracted from hydrogen gas and subjected to a series of acceleration stages, depicted in Figure~\ref{fig:LHC_scheme}~\cite{Lopienska:2800984}. Initially, LINAC4 accelerates hydrogen ions~(H$^-$) to \SI{160}{\mega\eV}. These ions then enter the Proton Synchrotron Booster, which strips the electrons, leaving only the protons and accelerates them to \SI{1.4}{\giga\eV}. From there, the protons are injected into the Proton Synchrotron (PS), reaching \SI{26}{\giga\eV}, and then into the Super Proton Synchrotron (SPS), which boosts them to \SI{450}{\giga\eV}. The SPS then injects the beams of protons into the LHC, where they are accelerated to their final energies by radio-frequency cavities and guided through the accelerator by superconducting magnets.
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/CCC-v2022-large.png}
    \caption{The CERN accelerator complex is a succession of machines, where each machine accelerates a beam of particles to a given energy before injecting the beam into the next one in the chain. This next machine brings the beam to an even higher energy and so on. The LHC is the last element of this chain, in which the beams reach their highest energies.}
    \label{fig:LHC_scheme}
\end{figure}
The accelerator contains over 1200 superconducting dipole magnets, each about 15 metres long, to bend the beams, while 392 quadrupole magnets maintain beam focus. To ensure optimal performance, these magnets are cooled to \SI{1.9}{\kelvin} using superfluid helium. The LHC operates with two beams of protons or heavy ions circulating in opposite directions in separate beam pipes. Proton beams within the LHC are divided into 2808 bunches, each containing approximately $10^{11}$ protons. These bunches are time-spaced by multiples of \SI{25}{\nano\second}, resulting in a bunch-crossing frequency of up to \SI{40}{\mega\hertz}, with an average bunch-crossing rate of approximately \SI{30}{\mega\hertz}. 
The LHC started operating in 2008 and collided the first beams at an energy of $\sqrt{s}=\SI{450}{\giga\eV}$ in 2009. Since then, it has undergone significant upgrades during its operation. The start of the physics research programme was marked by the beginning of Run~1 in 2010 with collisions at a centre-of-mass energy of  $\sqrt{s}=\SI{7}{\tera\eV}$, subsequently upgrading this energy to \SI{8}{\tera\eV} in 2012. After LS1, in 2015 the LHC began Run~2 colliding beams at a ground breaking energy of  $\sqrt{s}=\SI{13}{\tera\eV}$. During Run~3, begun in 2022, the LHC aims at reaching its peak design parameters, including a centre-of-mass energy of $\sqrt{s}=\SI{14}{\tera\eV}$ and an instantaneous luminosity of $\mathcal{L}\approx\SI{2e34}{\per\centi\meter\squared\per\second}$.

The high-energy beams collide at four designated points along the LHC ring, which house the major experiments: ATLAS, CMS, ALICE, and LHCb.
The experiments at the LHC serve different purposes. ATLAS and CMS are general-purpose detectors focusing on high-luminosity collisions to study a wide range of physics phenomena, including the search for new particles, extra dimensions, and dark matter. ALICE, a heavy-ion experiment, explores the quark-gluon plasma, a state of matter present shortly after the Big Bang. LHCb specializes in heavy-quark physics and operates at mid-range luminosity, focusing on flavor physics and $\mathcal{CP}$ violation studies.

\section{The LHCb Detector}\label{sec:detector}

The LHCb detector, depicted in Figure~\ref{fig:lhcb-detector}, is a single-arm forward spectrometer, with a pseudorapidity coverage of $2 \leq \eta \leq 5$. This unique design allows it to study particles containing heavy quarks, primarily bottom ($b$) and charm ($c$), which are typically produced at high pseudorapidity during proton-proton ($pp$) collisions. The LHCb coordinate system is a right handed system with positive $z$ running along the beam-line away from the interaction point and positive $y$ ``upward". From this it follows that positive $x$ points toward the cavern access (A-side) and away from the LHC cryogenics (C-side), as well as the LHC ring. The (0,0,0) point corresponds to the nominal interaction point position.%, located inside the VELO.

The LHCb underwent a significant upgrade during LS$2$ to accommodate increased luminosity associated with the upgraded LHC and to handle an elevated event rate.
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/UT-upgrade-detector-scaled.jpeg}
    \caption{Layout of LHCb-Upgrade I detector, side-view from the center of LHC ring.}
    \label{fig:lhcb-detector}
\end{figure}
With the upgrade, LHCb will operate at an average bunch-crossing rate of approximately \SI{30}{\mega\hertz}, with an average number of pp collisions per bunch-crossing anticipated to be around $\mu=5.5$. The expected peak value of the instantaneous luminosity is expected to be about \SI{2e33}{\per\centi\meter\squared\per\second}. As will later be explained in Section \ref{sec:lumi_levelling}, luminosity exponentially decreases during a Fill, due to beam intensity degradation effects. Unlike the other experiments at LHC, LHCb adopts a system called luminosity levelling to maintain the luminosity at a fixed value during a whole accelerator fill.  During Run~3 and Run~4, the experiment aims to collect at least \SI{50}{\per\femto\barn} of data, enabling unprecedented precision in measurements within the flavour sector of the Standard Model.

In order to sustain such high luminosity and to still maintain the capability of performing high precision measurements, key upgrades implemented for Run~3 included:
\begin{itemize}
\item \textbf{Tracking System}: The tracking system has been renewed, with a new silicon pixel detector, the Vertex Locator (VELO), which measures primary and secondary vertices\footnote{A Primary Vertex (PV) corresponds to the point where the initial $pp$  collision occurs, while a secondary vertex is the point in space where a particle produced in the $pp$ collision decays into other particles, eventually after travelling some distance from the PV.} positions with high precision. Additionally, a new silicon strips upstream tracker (UT) and a scintillating fibers tracker (SciFi) for track fitting were installed.

\item \textbf{Trigger System}: The trigger system has been completely overhauled. In Run~1 and Run~2, LHCb used a Level-zero (L$0$) hardware trigger, which reduced the data acquisition rate from \SI{40}{\mega\hertz} to \SI{1.1}{\mega\hertz}, based on information from the Calorimeter system and Muon stations. This system was removed after Run~2 in favour of a more flexible trigger fully based on software. The upgraded trigger system reconstructs the entire event in real-time since the first level, allowing to select which events to save based on the full decay tree. In order to process data from all sub-detector, also the data acquisition system was upgraded in order to readout each sub-detector at the full rate of \SI{40}{\mega\hertz}.
\end{itemize}
In the following sections an overview of each sub-detector will be given.

\subsection[VErtex LOcator]{VErtex LOcator $\bigl($VELO$\bigr)$}\label{sec:velo}
The Vertex Locator (VELO)~\cite{Bediaga:2013tje} is a silicon pixel detector designed to reconstruct primary and secondary decay vertices. In general, a silicon detector consists of a thin layer of silicon material, typically a few hundred micrometers thick, which is doped to create a $pn$ junction. As a bias voltage is applied, a depletion region is created. When charged particles pass through the silicon material, they ionise the atoms along their path, creating electron-hole pairs within the depleted region. The bias voltage accelerate then the charge carriers, which will create an avalanche. The now sizeable electric charge can be collected into the front-end electronics.  This signal is finally analysed to determine the location where a charged particle passed through the detector. Pixel detectors are a particular geometry of silicon detectors, as they implement a matrix of discrete sensing elements (pixels), allowing for particle hits to provide 2 coordinates, instead of the single one provided by the other most used silicon detector geometry, i.e. strip detectors. 
%The spacial resolution of the VELO needs to be better than the typical length of $b$ and $c$ hadrons, ranging from $100$ to \SI{500}{\micro\meter}.  

The VELO consists of 26 stations, which are pairs of retractable modules placed within the LHC vacuum pipe, in order to be as close as possible to the beam axis. 
A brief schematics of the geometry is reported in Figure~\ref{fig:velo-geometry}~\cite{LHCbVelo:2019flq}. Figure~\ref{fig_velo-xy} shows the behaviour of the two retractable modules in the fully closed (left) and fully open (right) configuration. During beam ramping, the modules are maintained at a safe position of approximately \SI{3}{\centi\meter} from the beamline (fully open), in order to prevent damage from instabilities of the high-energy beams in the LHC. As soon as stable beams are the declared, the VELO moves to its fully closed configuration. In this case, the innermost VELO sensor is just \SI{5.1}{\milli\meter} from the nominal interaction point, making it the closest detector to the beamline in any HEP experiment at CERN. 

A top view of the 26 stations is visible in Figure~\ref{fig_velo-side}, together with an illustration of the shape of the luminous region in the $xz$ plane and the LHCb acceptance for final-state particles given by the pseudorapidity range $2 \leq \eta \leq 5$. 

\begin{figure}
    \centering
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{figures/aperture.png}
    \caption{View of a station in the x-y plane. The two modules, VELO A on the right and VELO C on the left, can be opened and closed}\label{fig_velo-xy}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{figures/above_view.png}
    \caption{Top view of all the 52 modules in the x-z plane}\label{fig_velo-side}
    \end{subfigure}
    \caption{Two different schematics of the geometry of the VELO. Figure~\ref{fig_velo-xy} shows a view of the plane perpendicular to the beam pipe. The possible aperture of the two modules is also displayed. Figure~\ref{fig_velo-side} shows a view from the side, where we appreciate each side section of the VELO.}
    \label{fig:velo-geometry}
\end{figure}

Each module contains two tiles, and each tile has two sensors. Each sensor consists of 768~×~256 pixels, with a thickness of \SI{200}{\micro\meter}. 
The VELO sensors are read out by VeloPix ASICs, each featuring a 256~×~256 matrix of active silicon pixels with dimensions of \SI{55}{\micro\meter} × \SI{55}{\micro\meter}. The entire VELO system contains over 40 million pixels. The raw hit resolution varies between $9$ and \SI{15}{\micro\meter}, depending on the angle of the incoming particle. To maintain detection efficiency and account for highly inclined tracks, a 2-pixel wide overlap between sensors within the same tile is implemented. This overlap ensures no gaps in the detection coverage.
%designed to operate at high frequencies (up to \SI{40}{\mega\hertz}) and to withstand intense radiation levels.

At the trigger level, the VELO detects tracks with high impact parameter, which are indicative of heavy-flavored particle decays, helping reducing the minimum bias rate by providing initial track information. Offline, high-resolution vertex reconstruction is key to studying heavy-meson oscillations and $\mathcal{CP}$-dependent time asymmetries.

\subsection{Upstream Tracker}
The UT is a micro-strip silicon detector made of four layers~\cite{LHCb:2014uqj}. It replaced the former TT detector, retired due to irradiation. The UT comprises four detection layers, arranged to measure coordinates in a cross-pattern (X,U,V,X), with the orientation of the layers at angles relative to the vertical axis of $\SI{0}{\degree}$, $\SI{+5}{\degree}$, $\SI{-5}{\degree}$ and $\SI{0}{\degree}$, respectively. This structural design  creates a balanced configuration that enhances detection capabilities and minimises gaps in coverage. Each UT sensor is composed of \SI{250}{\micro\meter} thick silicon and a \SI{10}{\micro\meter} metalization layer. The segmentation is finer than the TT, with near-beam sensor pitch of \SI{95}{\micro\meter} and dimension of $98$×\SI{49}{\milli\meter\squared}, against the TT sensor pitch of \SI{183}{\micro\meter}. In Figure~\ref{fig:UT} a view of the detector is shown~\cite{ut}. 
The UT is a core part of the LHCb physics case, being required to constrain important decays such as $K_S \rightarrow\pi\pi$ that happen beyond the VELO acceptance. The UT also boosts the momentum resolution and helps identify ghost tracks.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/UT.png}
    \caption{Overview of the UT schematic layout}
    \label{fig:UT}
\end{figure}

\subsection{Magnet}
LHCb exploits the forward region of proton collisions and requires a dipole field with a free aperture of \SI{\pm300}{\milli\radian} horizontally and \SI{\pm250}{\milli\radian} vertically. In order to achieve this, LHCb has a magnet~\cite{LHCb:2000xej} consisting of two coils, both weighing 27 tonnes, mounted inside a 1450 tonne steel frame. Each coil is constructed from 10 layers, wound from almost 3000 metres of aluminium cable. A schematics of the magnet is depicted in Figure~\ref{fig:magnet}.
Tracking detectors in the magnetic field have to provide momentum measurement for charged particles with a precision of about 0.4\% for momenta up to \SI[per-mode=symbol]{200}{\giga\eVperc}. This demands an integrated field of \SI{4}{\tesla\meter} for tracks originating near the primary interaction point.
 The lateral aperture of the magnet is defined by the longitudinal extension of the detectors, placed upstream of the magnet. The two coils are each \SI{7.5}{\meter} long, \SI{4.6}{\meter} wide and \SI{2.5}{\meter} high.
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/lhcb-magnet.png}
    \caption{A schematic of the LHCb magnet (yellow) with the yoke (blue) supporting it}
    \label{fig:magnet}
\end{figure}
The LHCb magnet has a unique feature consisting into the possibility to reverse the polarity of the magnetic field (MagUp or MagDown). This allows a precise control of the charge asymmetries introduced by the detector. Particles hit preferentially one side of the detector, depending on their charges, generating potentially large detection asymmetries. If the data samples collected with the two different polarities have approximately equal size and if the operating conditions are stable enough, charge asymmetries are expected to cancel.

\subsection[Scintillating Fibre Tracker]{Scintillating Fibre Tracker$ \bigl($SciFi$\bigr)$}
The Scintillating Fibres (SciFi)~\cite{scifi} is another tracker sub-detector, that provides enhanced pattern recognition, momentum estimation, and overall tracking efficiency.
The SciFi is located downstream of the dipole magnet and consists of three tracking stations: T$1$, T$2$, and T$3$. AS the UT, each station of the SciFi comprises the four layers arranged in the (X,U,V,X) pattern described above.  This configuration offers high resolution in the bending plane of the magnetic field and supports efficient pattern recognition for charged particles. These layers are made up of modules measuring \SI{5}{\meter} by \SI{52}{\centi\meter}, constructed with $2.5$-meter-long scintillating fibres, \SI{250}{\micro\meter} in diameter. The innermost modules contains six fibre layers, while the others five, due to lower radiation levels.

A scheme of the detector geometry is reported in Figure~\ref{fig:scifi}.
%Scintillating fibres are made from a polymer core with $1\%$ by weight of fluorescent dye added to enhance scintillation. The fibres generate optical photons upon interaction with ionising radiation. This process involves depositing energy in the polymer core, which excites the material. Due to the inherent limitations of the base polymer in terms of light yield and relaxation time, the addition of the fluorescent dye improves efficiency by matching energy-level structures to enhance photon production.

%The SciFi uses Silicon Photomultipliers (SiPMs) to read the optical photons. These SiPMs are contained in read-out boxes located at the top and bottom of each detection plane, ensuring efficient collection and processing of the scintillation signals. The SiPMs offer high gain, fast response, and compactness, allowing them to be integrated into the compact design of the SciFi system.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/scifi.png}
    \caption{A schematics of the SciFi}
    \label{fig:scifi}
\end{figure}

The SciFi is designed for a high hit efficiency, with simulated performance indicating a hit efficiency of over 97\% at the end of its operational lifetime. The raw hit resolution is expected to be approximately \SI{42}{\micro\meter}, providing precise measurements of charged particle trajectories.

\subsection{RICH}
At LHCb there are two Ring Imaging Cherenkov (RICH) detectors, RICH1 and RICH2, that enable particle identification~\cite{LHCb:2013urp} across a broad momentum range, from $1$ to \SI[per-mode=symbol]{100}{\giga\eVperc}~\cite{Adinolfi_2013}.

Cherenkov radiation occurs when charged particles travel through a dielectric medium at speeds exceeding the local speed of light (superluminal speed). The angle at which Cherenkov radiation is emitted (Cherenkov angle) is directly related to the refractive index of the medium and the velocity of the particle. Given this relation, the Cherenkov angle $\theta_c$ can be used to infer the mass of a particle when its momentum is known:

\begin{equation}
    \cos\theta_c=\frac{1}{\beta n} = \frac{1}{n}\sqrt{1+\biggl(\frac{m}{p}\biggr)^2}
\end{equation}

where $\beta$ is the speed of the particle relative to the speed of light in natural units ($c=1$), $n$ is the refractive index of the medium, $m$ is the particle mass, and $p$ is its momentum.
This principle allows for the identification of different particles based on the Cherenkov angle. The RICH system uses this concept to separate charged hadrons over a momentum range of $1-\SI[per-mode=symbol]{100}{\giga\eVperc}$.
%, essential for studying hadronic final states and central to LHCb's physics goals, including precise measurements of $\mathcal{CP}$ violation and rare decays of $b$ and $c$ hadrons.

RICH$1$ is located between the VELO and the UT, upstream of the spectrometer magnet. It uses two radiators, aerogel (with refractive index $n=1.03$) and C4F10 (with refractive index $n=1.0014$), allowing discrimination of particles over a momentum range of $1-\SI[per-mode=symbol]{60}{\giga\eVperc}$. RICH$1$ covers an angular acceptance of $25-300$ mrad.

RICH$2$ is downstream of the spectrometer magnet, after the last T-station, covering an angular acceptance from $15$ to \SI{120}{\milli\radian} (non-bending plane) and $15$ to \SI{100}{\milli\radian} (bending plane). It uses CF4 (with refractive index $n=1.0005$) as the radiator, allowing discrimination of particles with momentum ranging from $15$ to $\SI[per-mode=symbol]{100}{\giga\eVperc}$. 
%The $\pi-K$ separation in RICH$2$ is approximately 90\% efficient for momenta up to \SI[per-mode=symbol]{30}{\giga\eVperc}.

The RICH detectors use spherical mirrors to focus Cherenkov light, with secondary flat mirrors to guide the photons onto Hybrid Photon Detectors (HPDs). These HPDs are sensitive to photons in the $200-\SI{600}{\nano\meter}$ wavelength range, and they are placed outside the detector acceptance, reducing material costs and protecting them from the spectrometer's magnetic field.

Arrays of multi-anode photomultiplier tubes (MaPMTs) are used to detect individual Cherenkov photons, with RICH$1$ having a total detection area of $\SI{1.6}{\squared\meter}$ and RICH$2$ having a detection area of $\SI{2.2}{\squared\meter}$. 
%This arrangement ensures efficient collection and processing of Cherenkov light for particle identification.

\begin{figure}
    \centering
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\linewidth]{figures/rich1.jpg}
    \caption{RICH$1$}\label{RICH1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=0.78\linewidth]{figures/rich2.png}
    \caption{RICH$2$}\label{rich2}
    \end{subfigure}
    \caption{Schematics side view of the two RICH detectors.}
    \label{fig:rich}
\end{figure}

\subsection{Calorimeters}
\sloppy
The calorimeter system in the LHCb experiment~\cite{LHCb:2000vji} plays its role in identifying electrons, photons, and hadrons, providing a measurement of their energies and positions. The system comprises the Electromagnetic Calorimeter (ECAL) and an  Hadron Calorimeter (HCAL). Both of them are  placed between the RICH$2$ and the second muon station, with an angular acceptance ranging from \SI{25}{\milli\radian} to \SI{250}{\milli\radian} in the bending plane and to \SI{300}{\milli\radian} in the non-bending plane. 
The ECAL is responsible for measuring the energy and position of electrons and photons. It is built with shashlik calorimeter technology\cite{Badier:293003}, consisting of alternating \SI{4}{\milli\meter}-thick polystyrene scintillating tiles and \SI{2}{\milli\meter} lead sheets. The scintillation light is collected using wavelength-shifting fibers and read out by individual photomultipliers (PMTs) mounted at the back of the tiles. The total thickness of the ECAL is 25 radiation lengths, providing sufficient thickness to contain most electromagnetic showers. The calorimeter structure is segmented into three zones depending on the radial distance from the beamline. The lateral segmentation of both ECAL and HCAL is shown in Figure~\ref{fig:cal_system}. The inner section has cells with a lateral dimension of $\SI{40.4}{\milli\meter}$, the middle section has cells with a $\SI{60.6}{\milli\meter}$ lateral dimension, and the outer section comprises cells with a $\SI{121.2}{\milli\meter}$ lateral dimension. 


The energy resolution for the ECAL is approximately 
 $\sigma_E/E~\text{(GeV)}~=~1\%~\oplus~10\%/\sqrt{E\text{(GeV)}}$.

\sloppy
The HCAL is responsible for measuring the energy and position of hadrons. It consists of \SI{16}{\milli\meter}-thick iron layers alternated with $\SI{2}{\milli\meter}$ scintillating fibres, with a total thickness corresponding to 5.6 nuclear interaction lengths. This design is constrained by the space available inside the LHCb cavern. The energy resolution for the HCAL is approximately $\sigma_E/E~\text{(GeV)}~=~9\%~\oplus~70\%~/\sqrt{E\text{(GeV)}}$. With this poor resolution, combined with the incapability of containing the full hadronic shower, the main function of HCAL was the estimation of hadron energy for the low level trigger L$0$ in Run~1 and Run~2. However, it is still used in some jets analysis, but it is planned to remove it during the next long shutdown.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/CAL_system.png}
    \caption{Lateral segmentation of the ECAL (left) and the HCAL (right). One quarter of the detector front face is shown.}
    \label{fig:cal_system}
\end{figure}
\subsection{Muon Stations}

The muon detector system primary function is to identify and measure the transverse momentum of muons in order to reconstruct decay channels involving muons, which are vital for many LHCb physics studies.
The LHCb muon detector system~\cite{Alves_2013} consists of four stations (M$2$ to M$5$) placed downstream of the hadronic calorimeter and interleaved with thick iron walls that act as muon filters. In Run~1 and Run~2, there was an additional M$1$ station located in front of the calorimeters, used for transverse momentum measurement for the L$0$ trigger~\cite{muon_upgrade}. The system covers the angular acceptance from \SI{20}{\milli\radian} to \SI{306}{\milli\radian} in the bending plane and from \SI{16}{\milli\radian} to \SI{258}{\milli\radian} in the non-bending plane. 
%Each station comprises two mechanically independent halves, called the A and C sides, which can be horizontally moved to access the beam pipe and detector chambers for installation and maintenance.
A scheme of the muon stations (including M1 no longer part of the detector) is depicted in Figure~\ref{fig:muon}.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/muon.png}
    \caption{A scheme of the muon stations. M$1$ was removed during Upgrade~1.}
    \label{fig:muon}
\end{figure}
The stations are equipped with Multi-Wire Proportional Chambers (MWPCs) for muon detection. These chambers use a gas mixture of argon, carbon dioxide, and tetrafluoromethane (CF4). The iron absorbers between the stations add filtering capacity, ensuring that only muons with momentum greater than \SI[per-mode=symbol]{6}{\giga\eVperc} can traverse the entire system.

Stations M$2$-M$3$ focus on transverse momentum measurements, while M$4$ and M$5$ serve to confirm if a particle has traversed the entire detector system. The system achieves a 95\% detection efficiency, providing robust muon identification. 

The unique feature of the LHCb muon detector system is its segmented structure, with each station divided into four regions (R$1$-R$4$), with cells scaling in the ratio 1:2:4:8 with distance from the beam axis. This segmentation allows the system to manage varying particle rates and optimise detection efficiency. 
%Additionally, the independent halves (A and C sides) offer flexibility during installation and maintenance, enabling quick access to the beam pipe and detector chambers when needed.


\section[Real Time Analysis]{Real Time Analysis $\bigl($RTA$\bigr)$}\label{sec:rta}

The demand for more accurate measurements has driven the collaboration to exploit the possibility to increase in instantaneous luminosity at the LHCb interaction point. This required to completely renew the detector readout and trigger systems. The upgraded system operates at an average event rate of \SI{30}{\mega\hertz}, processing every event with a fully software-based trigger system that integrates information from all sub-detectors to perform more accurate selections. The need for a more efficient trigger architecture emerged from simulation studies showing that the adopting in Run~3 the same trigger sequence used during Run~1 and Run~2 would lead to a loss of efficiency in hadronic B-meson decay channels as the instantaneous luminosity increased~\cite{CERN-LHCC-2011-001}. The first stage of the previous trigger system, L$0$, relied on transverse energy $E_T$ measurements from several sub-detectors. This resulted in high efficiencies for dimuon events but reduced efficiency for fully hadronic decays and for decays including electrons and photons in the final state. Additionally, the increase in the $E_T$ threshold to manage trigger rates with higher luminosities compromised signal efficiency, causing saturation of the trigger yield.

The renewed DAQ and trigger system at LHCb is designed to overcome these limitations~\cite{CERN-LHCC-2018-014}. The readout system can handle high input data flow, on the order of \SI[per-mode=symbol]{40}{\tera\bit\per\second}, while the software-based trigger can process events at an average rate of \SI{30}{\mega\hertz}, matching the average LHC bunch crossing rate.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/hidef_RTA_dataflow_widescreen.png}
    \caption{Workflow of the Real Time Analysis paradigm, from the detector readout to analysis production}
    \label{fig:RTA}
\end{figure}

The trigger system now implements a two-stage full software solution, schematised in Figure~\ref{fig:RTA}:
\begin{itemize}
\item HLT$1$: the first stage reduces the data rate by a factor of 30-60, by performing a partial reconstruction. It is executed on Graphics Processing Units (GPUs), installed on Event Builder (EB) nodes.
\item HLT$2$: the second stage, running on a farm of Central Processing Units (CPUs) allows for detailed event reconstruction with quasi-offline precision~\cite{Gazzoni:2670650}.
\end{itemize}
HLT$2$ has a very detailed precision, but it suffers from great slowness. In order to not lose the data, a buffer is implemented between HLT$1$ and HLT$2$, as can be seen in Figure~\ref{fig:RTA}. The events accumulate in this buffer, awaiting to be processed by HLT$2$. Due to this accumulation, the full event is processed hours or even days after the collision happened. 
The buffer also serves the purpose of data source for the Alignment \& Calibration phase of the detector. While the data awaits to be processed by HLT$2$, a subset of them is used to perform this task. Once available, the Alignment \& Calibration information is then back-propagated to all the events in HLT$1$, in order to enhance the resolution of estimated parameters (e.g. PVs), and also forth-propagated to HLT$2$ for the offline-quality level reconstruction.



The DAQ system~\cite{CERN-LHCC-2014-016} is composed of several sub-systems:
\begin{itemize}


\item The Event Builder (EB) collects information from all the sub-detectors and conveys the data regarding the same event to the same physical location. It additionally hosts the early stage reconstruction (HLT$1$) which is performed on GPUs mounted on the EB nodes.

\item The Event Filter Farm (EFF) hosts the farm of CPUs dedicated to performing the selection at HLT$2$ level.

\item The Experiment Control System (ECS) oversees the entire LHCb experiment, from infrastructure to data acquisition.

\item The Timing and Fast Control (TFC) system is responsible for distributing clock, timing, and trigger information to the front-end and readout systems. It synchronises to the LHC master clock, ensuring precise timing across all components. The TFC provides global signals, including the \SI{40}{\mega\hertz} clock, synchronous commands to control event processing, calibration commands for detectors, and electronics configuration distribution from the ECS.
\end{itemize}
In Figure~\ref{fig:architecture_readout} a scheme of the interaction between these elements is shown.
In Section \ref{sec:HLT1eHLT2} we present a more detailed description of the sub-systems relative to the HLT$1$ and HLT$2$, while the Alignment \& Calibration procedure is explained in Section \ref{sec:alignment}. Finally, in Section \ref{sec:ecs} we provide a description in ECS. 
%In the following subsections, a more detailed description of the sub-systems will be given.

\begin{figure}
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\linewidth]{figures/The-architecture-of-Upgraded-LHCb-readout-system.png}
    \caption{Architecture of the data readout and event builder system.}
    \label{fig:architecture_readout}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
         \includegraphics[width=\linewidth]{figures/RTA_trigger.png}
    \caption{Focus on the EB role and transmission lines within the DAQ workflow.}\label{fig:EB_zoom}
    \end{subfigure}
    \caption{Two views of the DAQ architecture and workflow. Each EB node receive information from a particular sub-detector. These information are exchanged using the EB network, in order to re-assign to each EB node the information of all the sub-detecotr of a specific event. Once this is done, the partial event reconstruction is performed on a set of GPUs, which reduce the throughput. The partial reconstructed event is partially stored in the online servers and forth-propagated to HLT for the second-stage trigger.}\label{fig:general_overview_trigger_RTA}
\end{figure}


\subsection{The HLT$1$ and HLT$2$ subsytems}\label{sec:HLT1eHLT2}
\subsubsection{Event Builder (EB)}
To collect data from all sub-detector channels and assemble them into complete event datasets, a process known as event building is employed. This entails gathering all data fragments corresponding to a single event and consolidating them centrally. This task is handled by a dedicated server farm called the Event-Builder (EB), which comprises 163 interconnected nodes. These nodes are linked to the detector front-end, other EB nodes, the buffer towards HLT$2$ farm, and the other control systems, such as TFC and ECS.

The workflow of the event building is as follows: 
The data from all the sub-detectors in the underground area are transmitted to the EB nodes on the surface through radiation-hard 300-meter-long optical fibres. Each EB node receives data from a specific sub-detector, and it is responsible for its readout. Each EB node receives data from a specific sub-detector, and it is responsible to collect from the other EB nodes data of the other sub-detectors concerning specific events. To do that, the EB nodes exchange information between them via the Event Builder Network. Once a EB node has concatenated data of a set of events from all the sub-detectors, it sends them to the HLT1 GPUs mounted onto the same motherboard.
Once the full event is reconstructed, the data is sent from the EB node to the buffer, awaiting for the full reconstruction by HLT2. 

In the next paragraph we give a description of the EB nodes architecture, in order to better understand how the EB nodes can perform the operations we just described. In the subsequent paragraph we present the operation that HLT$1$ performs within the GPUs mounted in the EB nodes. 

\paragraph{EB node architecture}
An overview of all the modules installed in a single EB network is schematised in Figure \ref{devices_EBnode}. The detector front-end is connected with $\sim10^{4}$ optical fibres to the readout boards, also referred to as TELL40 boards. These custom-built PCI Express (PCIe) boards are equipped with an Altera Arria 10 Field Programmable Logic Arrays (FPGA)\footnote{A more detailed description of the FPGA will be given in Section \ref{sec:FPGA}}, boasting one of the largest FPGA sizes available during their development phase. With a PCIe Gen3 interface, these boards can write data to host memory via Direct Memory Access (DMA) at speeds of up to $100$~Gbit/s. Each EB node executes multiple instances of a program, called Readout Unit (RU), responsible for transmitting event fragments to other EB nodes through the EB network, with the communication relying on 200~Gbit/s InfiniBand NICs (Network interface Controllers) in order to handle the high bandwidth. On the other hand, the Builder Unit (BU) on each node collects fragments corresponding to the same event. Each EB node has the capacity to accommodate up to three accelerator boards for data processing, with GPUs mounted on these slots to execute HLT$1$ operations. The events selected by HLT$1$ are then forwarded to the Event Filter Farm (EFF) via 10~Gbit/s Ethernet controllers, which are integrated into the server motherboard. 

\begin{figure}
\centering
    \includegraphics[width=0.5\textwidth]{figures/zoom_EB_node.png}
    \caption{Devices installed at each EB node, which comprehend three TELL40s (Readout), up to three GPUs (Accelerator) and communication ports for the other EB nodes (200G EB net) and EFF (10G HLT net).}\label{devices_EBnode}
\end{figure}

\paragraph{HLT$1$}
HLT$1$ runs on GPUs hosted on the same motherboards as the EB nodes. It significantly reduces the data rate, by a factor of 30-60, enabling the use of a smaller and more cost-effective network between the EB and the EFF. This reduction is crucial considering the immense bandwidth requirement of raw data, of about 40~Tbit/s.

The HLT$1$ stage aims to reject events that do not contain particles of interest while retaining those that do. 
It uses the full-offline charged particle reconstruction, and it implements inclusive single or two-track selections to determine the event interest. 
%HLT$1$ is entirely implemented on GPUs in the same servers hosting the Event Builder. 
The \textit{Allen} application, written in C++ with CUDA extensions, executes the following tasks~\cite{CERN-LHCC-2020-006}:
\begin{itemize}
\item Decoding raw detector data into the LHCb global coordinate system
\item Clustering of detector hits
\item Pattern recognition to identify hit combinations associated with the same particle
\item Fitting track candidates using a Kalman Filter to estimate momentum and other parameters
\item Reconstructing primary and secondary vertices from fitted tracks
\item Taking trigger decisions based on track parameters like impact parameter and momentum
\end{itemize}
Once the events are selected, they are propagated to the Event Filter Farm. 


\subsubsection{Event Filter Farm (EFF)}

The EFF is exclusively dedicated to running HLT2 and it comprises a heterogeneous mixture of CPU servers of various generations and different core counts. 
%Given the asynchronous nature of HLT2 processing, load balancing among these servers is handled as an implementation detail.
While HLT$1$ makes a first-stage cut on events based on long tracks (i.e. created near the interaction point) information, HLT$2$ includes a more comprehensive event analysis using information from all the sub-detectors as the RICH. 
This allows also the reconstruction of others kinds of tracks as the downstream, i.e. tracks generated outside of the VELO acceptance. This information is crucial, because long-lived neutral particles, such as the $K^0_S$ and $\Lambda^0$, decay mostly outside the acceptance of the VELO.
%Furthermore, HLT$2$ accounts for information of sub-detectors that are not used in the HLT$1$ phase, such as the RICH.

\subsection{The Alignment and Calibration phase}\label{sec:alignment}
This procedure distinguishes between two phases~\cite{Dziurda:2640712}:
\begin{itemize}
\item the Alignment focuses on correcting shifts and rotations in the detector's components, ensuring that the data obtained from these detectors are accurate. This process is applied to tracking detectors like the VELO, UT, SciFi, and Muon system, ensuring that their positioning aligns with the expected geometry;
\item the Calibration involves fine-tuning the responses of various sub-detectors to ensure accurate measurements. This includes calibrating the RICH, ECAL, and HCAL.
\end{itemize}
Both processes are triggered at the beginning of each LHCb run, and they are fully integrated into the LHCb control system. 
\paragraph{Alignment}
Alignment at LHCb involves minimising the chi-square $\chi^2$ of all tracks with respect to the alignment parameters $\alpha$, which account for translations and rotations of detector elements. The procedure employs the iterative Newton-Raphson method\cite{raphson2011analysis}, where the first and second derivatives of $\chi^2$ with respect to $\alpha$ are calculated to determine the optimal adjustments needed for alignment. The data for this alignment procedure comes from tracks obtained through a Kalman filter used in track reconstruction~\cite{HULSBERGEN2009471}, with the option of adding vertex position and mass constraints for increased precision.

The alignment process has two key components~\cite{Saur:20230E}:
\begin{itemize}
\item Analyser: This part runs the track reconstruction, computes the $\chi^2$ derivatives, and saves them to binary files. It uses multi-threaded reconstruction based on 163 nodes, allowing for efficient data processing.
\item Iterator: This component collects the derivatives from the Analyser, performs the minimisation step, and checks for convergence. If there is a significant difference between the previous and current alignment constants, the updated constants are used in the HLT2 trigger, ensuring that subsequent data collection reflects the corrected alignment.
\end{itemize}

Not only the trackers, but also the RICH mirrors need to be aligned. This is done by creating histograms of the difference ($\Delta \theta_c$ ) between a detected photon reconstructed Cherenkov angle and its expected one. By fitting the appropriate distribution and correcting for deviations of its mean value from zero in each bin, the RICH mirrors can be aligned. This procedure can take several hours, in contrast to the relative quick ($\sim$10 minutes) tracker alignment.
\paragraph{Calibration}
Calibration at LHCb involves ensuring that the detector responses are accurate and consistent. It includes:
\begin{itemize}
\item RICH Calibration: This involves evaluating the refractive index, that depends on the RICH temperature and gas pressure;
\item Calorimeter Calibration: ECAL and HCAL calibration focuses on ensuring accurate energy measurements.
\end{itemize}
The calibration process leverages dedicated HLT$1$ trigger lines to select events suitable for calibration and alignment. These events are stored in a buffer until sufficient data are accumulated to run the calibration and alignment procedure. This buffering allows the reconstruction algorithms running at HLT$2$ to use the most accurate and relevant alignment and calibration constants.


\subsection{Experiment Control System (ECS)}\label{sec:ecs}

The ECS~\cite{GranadoCardoso:2702137} oversees the entire LHCb experiment, from infrastructure to data acquisition. It is a distributed system based on the commercial Supervisory Control And Data Acquisition (SCADA) system WinCC OA, and custom-developed components. The ECS features user interfaces, data archiving, alarm handling, and various device interfaces. Its tree-like control hierarchy, based on a Finite State Machine (FSM), allows commands to be propagated downstream and state information to be propagated upward, as depicted in Figure~\ref{fig:ECS_tree}.
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/ECS_tree.png}
    \caption{ECS schematic structure. The topmost node (the ECS itself) propagates commands to the downstream units (FE, DAQ systems, Trigger, ...) and it interfaces with the LHC system.}
    \label{fig:ECS_tree}
\end{figure}

Of particular interest for this thesis is that ECS provides simultaneous access to all TELL40 boards. As will later be discussed, this feature is important in order to combine information from the different readout boards of the VELO, allowing the work done in this thesis. To automate the calculations developed in this thesis and ensure regularity, a WinCC control script is developed in the context of the VELO WinCC project. The WinCC script allows to export \textit{datapoints} representing the value of the quantity of interest at specific timestamps. The LHCb online monitoring system application,~\textit{Monet}, can display them in trending plots or histograms accessible from any browser~\cite{Adinolfi:2298467}.

ECS allows to view and manage the entire LHCb experiment from a central location, at any moment. An important component of ECS is the Run Control, which oversees all tasks essential for data collection. The Run Control is in charge of setting up the sub-detectors front-end electronics, bias voltage system and readout boards, the TFC system, and for initialising and configuring all necessary DAQ processes. 
This structure provides centralised control and monitoring, ensuring efficient management of the LHCb experiment.