%!TEX root = ../dissertation.tex
For the ongoing LHC Run~3, LHCb has introduced an innovative trigger system that employs full software reconstruction of every collision event, marking the first time that a \SI{40}{\mega\hertz} stream of LHC collision events is filtered based on offline-quality event reconstruction performed in real time. This system processes a data flow of approximately \SI{40}{\tera\bit\per\second}, posing significant computational challenges. To address this, LHCb adopted a heterogeneous computing system utilizing CPUs, GPUs, and FPGAs simultaneously. As LHCb plans to introduce additional computing power at an even earlier stage for Run~4, a 2D cluster-finding algorithm was developed and is already operational in Run~3. This algorithm runs on FPGAs and determines the coordinates of all hits in the VELO, LHCb pixel detector for vertex reconstruction.

The goal of this thesis is to explore the potential advantages of real-time accessibility to an unprecedented flow of $\sim4 \times 10^{10}$ hits per second from a precision detector like the VELO. I focused on applications that could be practically implemented with the limited residual processing power available within the LHCb readout system, ensuring no impact on throughput. This involves using simple statistical methods, such as counting rates of reconstructed hits in specific VELO regions.

By appropriately combining the 208 implemented counters, I evaluated seven linear estimators: a luminosity estimate, the average position of the luminous region in the transverse plane ($x$ and $y$ coordinates), and the average positions of the two VELO halves in both transverse components. The luminosity measurement was calibrated using a Van Der Meer scan and a trimmed mean of different luminosity counters, achieving a statistical precision of $0.3\%$, which is currently the best available in real-time at LHCb.

Leveraging Principal Component Analysis, I developed a method to combine the counters for monitoring the luminous region and VELO positions. This process involves a simple scalar product between the online counters and weights estimated offline through Monte Carlo simulations, allowing fast and accurate calculations with a resolution of \SI{4}{\micro\meter} every few milliseconds. 

These results demonstrate the significant benefits of high-throughput computing at the early stages of data processing using specialised computing devices, encouraging further exploration of these technologies in future experiments.